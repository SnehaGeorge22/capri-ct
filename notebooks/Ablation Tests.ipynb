{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b72578",
   "metadata": {},
   "source": [
    "## CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality Optimization in Computed Tomography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253bb80",
   "metadata": {},
   "source": [
    "### Ablation Testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abe292",
   "metadata": {},
   "source": [
    "Ablation testing is like a \"what-if\" experiment for AI models. It involves removing one part of the model or one input at a time to see how much it affects the results. If the model performs much worse after removing something, that part was probably important. This helps us understand which features or components matter most for the model’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce046",
   "metadata": {},
   "source": [
    "So, In this ablation testing , we will creating different variants of our CAPRI-CT Causal Aware model in terms of the input parameters passed to the model and provide the analysis results at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82439dd",
   "metadata": {},
   "source": [
    "Below are the variants tested in this notebook: \n",
    "\n",
    "- Image(i) + Metadata (v, t, a) (full model)\n",
    "- Image(i) + Metadata (v, t, a, noise) (Robustness)\n",
    "- Image(i) + Metadata (v, a) (without Current per time)\n",
    "- Image(i) + Metadata (t, a) (without voltage)\n",
    "- Image(i) only (without voltage, current, agent)\n",
    "- Image(i) + Metadata (v, t) (without agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee122713",
   "metadata": {},
   "source": [
    "This Ablation testing can also be termed as Causal graph perturbation where we are removing each input node in the causal graph and test our assumptions made in the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a9f45",
   "metadata": {},
   "source": [
    "<img src=\"..\\images\\capri-ct-dag.png\" alt=\"Causal Graph\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c2ff5",
   "metadata": {},
   "source": [
    "Lets look into the each variant one by one !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Importing the required libraries for Ablation testing\n",
    "#######################################################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler, Subset, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e75f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# Below is the Class CapriCTDataset \n",
    "# Combining the CT image with the metadata using Dataset package\n",
    "# for SNR prediction\n",
    "##########################################################################################\n",
    "\n",
    "class CapriCTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for CT scan images combined with metadata and target SNR values.\n",
    "\n",
    "    Each sample consists of a grayscale CT image, encoded metadata (Voltage, Time, Contrast Agent),\n",
    "    and a corresponding SNR value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_csv : str\n",
    "        Path to the CSV file containing metadata for each image.\n",
    "        Expected columns: 'Filename', 'Voltage', 'Time', 'Classification', 'SNR'.\n",
    "    \n",
    "    img_folder_path : str\n",
    "        Path to the folder containing the CT scan image files.\n",
    "    \n",
    "    transform : callable, optional\n",
    "        Optional transform to be applied on a sample image (e.g., resizing, normalization).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    agent_dict : dict\n",
    "        Mapping of contrast agent labels to integer indices.\n",
    "    \n",
    "    voltage_dict : dict\n",
    "        Mapping of voltage levels to integer indices.\n",
    "    \n",
    "    time_dict : dict\n",
    "        Mapping of scan time values to integer indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metadata_csv, img_folder_path, transform=None):\n",
    "        self.img_data = pd.read_csv(metadata_csv)\n",
    "        self.img_folder = img_folder_path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Mappings\n",
    "        self.agent_dict = {'Iodine': 0, 'BiNPs 50nm': 1, 'BiNPs 100nm': 2}\n",
    "        self.voltage_dict = {80: 0, 100: 1, 120: 2, 140: 3}\n",
    "        self.time_dict = {215: 0, 430: 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.img_data.iloc[idx]\n",
    "\n",
    "        # Load and transform image\n",
    "        img_path = os.path.join(self.img_folder, row['Filename'])\n",
    "        img = Image.open(img_path).convert('L')  \n",
    "        image = self.transform(img) if self.transform else img\n",
    "\n",
    "        # Convert categorical fields to indices\n",
    "        voltage_idx = torch.tensor(self.voltage_dict[row['Voltage']], dtype=torch.long)\n",
    "        time_idx = torch.tensor(self.time_dict[row['Time']], dtype=torch.long)\n",
    "        agent_idx = torch.tensor(self.agent_dict[row['Classification']], dtype=torch.long)\n",
    "\n",
    "        # Target SNR\n",
    "        snr = torch.tensor(row['SNR'], dtype=torch.float32)\n",
    "\n",
    "        return image, voltage_idx, time_idx, agent_idx, snr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65049ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Below is our CAPRI-CT Causal VAE model\n",
    "###############################################################################\n",
    "\n",
    "class CapriCTCausalVAEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Causal Variational Autoencoder (VAE) model for predicting SNR from CT images and metadata.\n",
    "\n",
    "    This model combines image features with categorical metadata embeddings (Voltage, Time, Agent)\n",
    "    and uses a VAE structure to learn a low-dimensional latent representation for regression tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latent_dim : int, optional\n",
    "        Dimensionality of the latent space. Default is 64.\n",
    "    voltage_classes : int, optional\n",
    "        Number of distinct voltage classes. Default is 4.\n",
    "    time_classes : int, optional\n",
    "        Number of distinct time classes. Default is 2.\n",
    "    agent_classes : int, optional\n",
    "        Number of distinct contrast agent classes. Default is 3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=64, voltage_classes=4, time_classes=2, agent_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.voltage_embed = nn.Embedding(voltage_classes, 16)\n",
    "        self.time_embed = nn.Embedding(time_classes, 8)\n",
    "        self.agent_embed = nn.Embedding(agent_classes, 12)\n",
    "\n",
    "        # CNN Encoder\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout_cnn = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Fully connected layers for VAE\n",
    "        conv_output_size = 128 * 3 * 3  # Assuming 9x9 input image\n",
    "        embed_size = 16 + 8 + 12\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_output_size + embed_size, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + embed_size, 128)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 64)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
    "        self.decoder_out = nn.Linear(64, 1)\n",
    "\n",
    "    def encode(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Encodes the input image and metadata into latent mean and log-variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices of shape (B,).\n",
    "        time_idx : Tensor\n",
    "            Time class indices of shape (B,).\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices of shape (B,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor\n",
    "            Mean of the latent Gaussian distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.bn1(self.conv1(img)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = self.dropout_cnn(h)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "\n",
    "        # Embeddings\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, t_emb, a_emb], dim=1)\n",
    "\n",
    "        h_combined = torch.cat([h, emb], dim=1)\n",
    "        h1 = F.relu(self.bn_fc1(self.fc1(h_combined)))\n",
    "        h1 = self.dropout_fc(h1)\n",
    "\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) using standard normal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : Tensor\n",
    "            Mean of the latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Sampled latent vector z.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector and metadata into the predicted SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : Tensor\n",
    "            Latent vector of shape (B, latent_dim).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        \"\"\"\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, t_emb, a_emb], dim=1)\n",
    "\n",
    "        z_combined = torch.cat([z, emb], dim=1)\n",
    "        h = F.relu(self.bn_dec1(self.decoder_fc1(z_combined)))\n",
    "        h = F.relu(self.bn_dec2(self.decoder_fc2(h)))\n",
    "        snr_pred = self.decoder_out(h)\n",
    "        return snr_pred.squeeze(1)\n",
    "\n",
    "    def forward(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes input, samples from latent space, and decodes to predict SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snr_pred : Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        mu : Tensor\n",
    "            Mean of latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of latent distribution.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(img, voltage_idx, time_idx, agent_idx)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        snr_pred = self.decode(z, voltage_idx, time_idx, agent_idx)\n",
    "        return snr_pred, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Below is one of CAPRI-CT Causal VAE model variant without metadata\n",
    "###############################################################################\n",
    "\n",
    "class CapriCTWithoutEmbedModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE-based model for predicting SNR from CT images without using metadata embeddings.\n",
    "\n",
    "    The model uses a CNN encoder to extract features from the image, maps them to a \n",
    "    latent space, and decodes the latent vector to predict the SNR value.\n",
    "\n",
    "    Args:\n",
    "        latent_dim (int): Dimensionality of the latent space (default: 64).\n",
    "        voltage_classes (int): Number of voltage levels (unused).\n",
    "        time_classes (int): Number of time levels (unused).\n",
    "        agent_classes (int): Number of contrast agent types (unused).\n",
    "\n",
    "    Methods:\n",
    "        encode(img): Encodes the input image into mean and log variance.\n",
    "        reparameterize(mu, logvar): Samples from the latent space using the reparameterization trick.\n",
    "        decode(z): Decodes the latent vector into SNR.\n",
    "        forward(img, voltage_idx, time_idx, agent_idx): Full forward pass (metadata unused).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=64, voltage_classes=4, time_classes=2, agent_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        #self.voltage_embed = nn.Embedding(voltage_classes, 16)\n",
    "        #self.time_embed = nn.Embedding(time_classes, 8)\n",
    "        #self.agent_embed = nn.Embedding(agent_classes, 12)\n",
    "\n",
    "        # CNN Encoder with more depth and dropout\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout_cnn = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Flattened size for input image 9x9 → (128, 3, 3)\n",
    "        conv_output_size = 128 * 3 * 3\n",
    "        embed_size = 0\n",
    "\n",
    "        # Latent space\n",
    "        self.fc1 = nn.Linear(conv_output_size + embed_size, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + embed_size, 128)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 64)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
    "        self.decoder_out = nn.Linear(64, 1)\n",
    "\n",
    "    def encode(self, img):\n",
    "        \"\"\"\n",
    "        Encodes the input image and metadata into latent mean and log-variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor\n",
    "            Mean of the latent Gaussian distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.bn1(self.conv1(img)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = self.dropout_cnn(h)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "\n",
    "        # Embeddings\n",
    "        #v_emb = self.voltage_embed(voltage_idx)\n",
    "        #t_emb = self.time_embed(time_idx)\n",
    "        #a_emb = self.agent_embed(agent_idx)\n",
    "        #emb = torch.cat([v_emb, t_emb, a_emb], dim=1)\n",
    "\n",
    "        #h_combined = torch.cat([h, emb], dim=1)\n",
    "        h1 = F.relu(self.bn_fc1(self.fc1(h)))\n",
    "        h1 = self.dropout_fc(h1)\n",
    "\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) using standard normal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : Tensor\n",
    "            Mean of the latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Sampled latent vector z.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector and metadata into the predicted SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : Tensor\n",
    "            Latent vector of shape (B, latent_dim).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        \"\"\"\n",
    "        #v_emb = self.voltage_embed(voltage_idx)\n",
    "        #t_emb = self.time_embed(time_idx)\n",
    "        #_emb = self.agent_embed(agent_idx)\n",
    "        #emb = torch.cat([v_emb, t_emb, a_emb], dim=1)\n",
    "\n",
    "        #z_combined = torch.cat([z, emb], dim=1)\n",
    "        h = F.relu(self.bn_dec1(self.decoder_fc1(z)))\n",
    "        h = F.relu(self.bn_dec2(self.decoder_fc2(h)))\n",
    "        snr_pred = self.decoder_out(h)\n",
    "        return snr_pred.squeeze(1)\n",
    "\n",
    "    def forward(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes input, samples from latent space, and decodes to predict SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices. Unused\n",
    "        time_idx : Tensor\n",
    "            Time class indices. Unused\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices. Unused\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snr_pred : Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        mu : Tensor\n",
    "            Mean of latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of latent distribution.\n",
    "\n",
    "        Note:\n",
    "        Even the voltage_idx, time_idx, agent_idx are passed , they are unused\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(img)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        snr_pred = self.decode(z)\n",
    "        return snr_pred, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68570a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Below is one of CAPRI-CT Causal VAE model variant without Time input\n",
    "###############################################################################\n",
    "\n",
    "class CapriCTWithoutTimeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE-based model for predicting SNR from CT images using voltage and agent embeddings,\n",
    "    excluding time as an input feature.\n",
    "\n",
    "    The model encodes image features and metadata (voltage, agent) into a latent space \n",
    "    and decodes it to predict the SNR value.\n",
    "\n",
    "    Args:\n",
    "        latent_dim (int): Size of the latent vector (default: 64).\n",
    "        voltage_classes (int): Number of voltage categories.\n",
    "        time_classes (int): Number of time categories (unused).\n",
    "        agent_classes (int): Number of contrast agent types.\n",
    "\n",
    "    Methods:\n",
    "        encode(img, voltage_idx, time_idx, agent_idx): Encodes the image and metadata \n",
    "            (excluding time) into latent mean and log variance.\n",
    "        reparameterize(mu, logvar): Samples latent vector using reparameterization trick.\n",
    "        decode(z, voltage_idx, time_idx, agent_idx): Decodes latent vector and metadata \n",
    "            (excluding time) into predicted SNR.\n",
    "        forward(img, voltage_idx, time_idx, agent_idx): Executes full VAE pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64, voltage_classes=4, time_classes=2, agent_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.voltage_embed = nn.Embedding(voltage_classes, 16)\n",
    "        self.time_embed = nn.Embedding(time_classes, 8)\n",
    "        self.agent_embed = nn.Embedding(agent_classes, 12)\n",
    "\n",
    "        # CNN Encoder with more depth and dropout\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout_cnn = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Flattened size for input image 9x9 → (128, 3, 3)\n",
    "        conv_output_size = 128 * 3 * 3\n",
    "        embed_size = 16 + 12\n",
    "\n",
    "        # Latent space\n",
    "        self.fc1 = nn.Linear(conv_output_size + embed_size, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + embed_size, 128)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 64)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
    "        self.decoder_out = nn.Linear(64, 1)\n",
    "\n",
    "    def encode(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Encodes the input image and metadata into latent mean and log-variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices of shape (B,).\n",
    "        time_idx : Tensor\n",
    "            Time class indices of shape (B,). Unused\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices of shape (B,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor\n",
    "            Mean of the latent Gaussian distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.bn1(self.conv1(img)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = self.dropout_cnn(h)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "\n",
    "        # Embeddings\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        #t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, a_emb], dim=1)\n",
    "\n",
    "        h_combined = torch.cat([h, emb], dim=1)\n",
    "        h1 = F.relu(self.bn_fc1(self.fc1(h_combined)))\n",
    "        h1 = self.dropout_fc(h1)\n",
    "\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) using standard normal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : Tensor\n",
    "            Mean of the latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Sampled latent vector z.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector and metadata into the predicted SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : Tensor\n",
    "            Latent vector of shape (B, latent_dim).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices. Unused\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        \"\"\"\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        #t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, a_emb], dim=1)\n",
    "\n",
    "        z_combined = torch.cat([z, emb], dim=1)\n",
    "        h = F.relu(self.bn_dec1(self.decoder_fc1(z_combined)))\n",
    "        h = F.relu(self.bn_dec2(self.decoder_fc2(h)))\n",
    "        snr_pred = self.decoder_out(h)\n",
    "        return snr_pred.squeeze(1)\n",
    "\n",
    "    def forward(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes input, samples from latent space, and decodes to predict SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snr_pred : Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        mu : Tensor\n",
    "            Mean of latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of latent distribution.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(img, voltage_idx, time_idx, agent_idx)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        snr_pred = self.decode(z, voltage_idx, time_idx, agent_idx)\n",
    "        return snr_pred, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Below is one of CAPRI-CT Causal VAE model variant without Voltage input\n",
    "###############################################################################\n",
    "\n",
    "class CapriCTWithoutVoltageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE-based model for predicting SNR from CT images using time and agent embeddings,\n",
    "    excluding voltage as an input feature.\n",
    "\n",
    "    The model encodes image features and metadata (time, agent) into a latent space \n",
    "    and decodes it to predict the SNR value.\n",
    "\n",
    "    Args:\n",
    "        latent_dim (int): Size of the latent vector (default: 64).\n",
    "        voltage_classes (int): Number of voltage categories. (unused)\n",
    "        time_classes (int): Number of time categories \n",
    "        agent_classes (int): Number of contrast agent types.\n",
    "\n",
    "    Methods:\n",
    "        encode(img, voltage_idx, time_idx, agent_idx): Encodes the image and metadata \n",
    "            (excluding voltage) into latent mean and log variance.\n",
    "        reparameterize(mu, logvar): Samples latent vector using reparameterization trick.\n",
    "        decode(z, voltage_idx, time_idx, agent_idx): Decodes latent vector and metadata \n",
    "            (excluding voltage) into predicted SNR.\n",
    "        forward(img, voltage_idx, time_idx, agent_idx): Executes full VAE pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=64, voltage_classes=4, time_classes=2, agent_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.voltage_embed = nn.Embedding(voltage_classes, 16)\n",
    "        self.time_embed = nn.Embedding(time_classes, 8)\n",
    "        self.agent_embed = nn.Embedding(agent_classes, 12)\n",
    "\n",
    "        # CNN Encoder with more depth and dropout\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout_cnn = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Flattened size for input image 9x9 → (128, 3, 3)\n",
    "        conv_output_size = 128 * 3 * 3\n",
    "        embed_size = 8 + 12\n",
    "\n",
    "        # Latent space\n",
    "        self.fc1 = nn.Linear(conv_output_size + embed_size, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + embed_size, 128)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 64)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
    "        self.decoder_out = nn.Linear(64, 1)\n",
    "\n",
    "    def encode(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Encodes the input image and metadata into latent mean and log-variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices of shape (B,). Unused\n",
    "        time_idx : Tensor\n",
    "            Time class indices of shape (B,).\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices of shape (B,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor\n",
    "            Mean of the latent Gaussian distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.bn1(self.conv1(img)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = self.dropout_cnn(h)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "\n",
    "        # Embeddings\n",
    "        #v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([ t_emb, a_emb], dim=1)\n",
    "\n",
    "        h_combined = torch.cat([h, emb], dim=1)\n",
    "        h1 = F.relu(self.bn_fc1(self.fc1(h_combined)))\n",
    "        h1 = self.dropout_fc(h1)\n",
    "\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) using standard normal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : Tensor\n",
    "            Mean of the latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Sampled latent vector z.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector and metadata into the predicted SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : Tensor\n",
    "            Latent vector of shape (B, latent_dim).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices. Unused\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        \"\"\"\n",
    "        #v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([ t_emb, a_emb], dim=1)\n",
    "\n",
    "        z_combined = torch.cat([z, emb], dim=1)\n",
    "        h = F.relu(self.bn_dec1(self.decoder_fc1(z_combined)))\n",
    "        h = F.relu(self.bn_dec2(self.decoder_fc2(h)))\n",
    "        snr_pred = self.decoder_out(h)\n",
    "        return snr_pred.squeeze(1)\n",
    "\n",
    "    def forward(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes input, samples from latent space, and decodes to predict SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snr_pred : Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        mu : Tensor\n",
    "            Mean of latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of latent distribution.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(img, voltage_idx, time_idx, agent_idx)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        snr_pred = self.decode(z, voltage_idx, time_idx, agent_idx)\n",
    "        return snr_pred, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fe403",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Below is one of CAPRI-CT Causal VAE model variant without Agent input\n",
    "###############################################################################\n",
    "\n",
    "class CapriCTWithoutAgentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE-based model for predicting SNR from CT images using voltage and time embeddings,\n",
    "    excluding agent as an input feature.\n",
    "\n",
    "    The model encodes image features and metadata (voltage, time) into a latent space \n",
    "    and decodes it to predict the SNR value.\n",
    "\n",
    "    Args:\n",
    "        latent_dim (int): Size of the latent vector (default: 64).\n",
    "        voltage_classes (int): Number of voltage categories.\n",
    "        time_classes (int): Number of time categories .\n",
    "        agent_classes (int): Number of contrast agent types (unused)\n",
    "\n",
    "    Methods:\n",
    "        encode(img, voltage_idx, time_idx, agent_idx): Encodes the image and metadata \n",
    "            (excluding agent) into latent mean and log variance.\n",
    "        reparameterize(mu, logvar): Samples latent vector using reparameterization trick.\n",
    "        decode(z, voltage_idx, time_idx, agent_idx): Decodes latent vector and metadata \n",
    "            (excluding agent) into predicted SNR.\n",
    "        forward(img, voltage_idx, time_idx, agent_idx): Executes full VAE pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64, voltage_classes=4, time_classes=2, agent_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.voltage_embed = nn.Embedding(voltage_classes, 16)\n",
    "        self.time_embed = nn.Embedding(time_classes, 8)\n",
    "        self.agent_embed = nn.Embedding(agent_classes, 12)\n",
    "\n",
    "        # CNN Encoder with more depth and dropout\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout_cnn = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Flattened size for input image 9x9 → (128, 3, 3)\n",
    "        conv_output_size = 128 * 3 * 3\n",
    "        embed_size = 16 + 8\n",
    "\n",
    "        # Latent space\n",
    "        self.fc1 = nn.Linear(conv_output_size + embed_size, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + embed_size, 128)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 64)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
    "        self.decoder_out = nn.Linear(64, 1)\n",
    "\n",
    "    def encode(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Encodes the input image and metadata into latent mean and log-variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices of shape (B,).\n",
    "        time_idx : Tensor\n",
    "            Time class indices of shape (B,).\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices of shape (B,). Unused\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor\n",
    "            Mean of the latent Gaussian distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.bn1(self.conv1(img)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = self.dropout_cnn(h)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "\n",
    "        # Embeddings\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        #a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, t_emb], dim=1)\n",
    "\n",
    "        h_combined = torch.cat([h, emb], dim=1)\n",
    "        h1 = F.relu(self.bn_fc1(self.fc1(h_combined)))\n",
    "        h1 = self.dropout_fc(h1)\n",
    "\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) using standard normal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : Tensor\n",
    "            Mean of the latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Sampled latent vector z.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector and metadata into the predicted SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : Tensor\n",
    "            Latent vector of shape (B, latent_dim).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices. Unused\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        \"\"\"\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        #a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, t_emb], dim=1)\n",
    "\n",
    "        z_combined = torch.cat([z, emb], dim=1)\n",
    "        h = F.relu(self.bn_dec1(self.decoder_fc1(z_combined)))\n",
    "        h = F.relu(self.bn_dec2(self.decoder_fc2(h)))\n",
    "        snr_pred = self.decoder_out(h)\n",
    "        return snr_pred.squeeze(1)\n",
    "\n",
    "    def forward(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes input, samples from latent space, and decodes to predict SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snr_pred : Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        mu : Tensor\n",
    "            Mean of latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of latent distribution.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(img, voltage_idx, time_idx, agent_idx)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        snr_pred = self.decode(z, voltage_idx, time_idx, agent_idx)\n",
    "        return snr_pred, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Below is one of CAPRI-CT Causal VAE model variant with added noise\n",
    "###############################################################################\n",
    "\n",
    "class CapriCTRobustCheckModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Causal Variational Autoencoder (VAE) model for predicting SNR from CT images and metadata + noise.\n",
    "\n",
    "    This model combines image features with categorical metadata embeddings (Voltage, Time, Agent)\n",
    "    and uses a VAE structure to learn a low-dimensional latent representation for regression tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latent_dim : int, optional\n",
    "        Dimensionality of the latent space. Default is 64.\n",
    "    voltage_classes : int, optional\n",
    "        Number of distinct voltage classes. Default is 4.\n",
    "    time_classes : int, optional\n",
    "        Number of distinct time classes. Default is 2.\n",
    "    agent_classes : int, optional\n",
    "        Number of distinct contrast agent classes. Default is 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=64, voltage_classes=4, time_classes=2, agent_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.voltage_embed = nn.Embedding(voltage_classes, 16)\n",
    "        self.time_embed = nn.Embedding(time_classes, 8)\n",
    "        self.agent_embed = nn.Embedding(agent_classes, 12)\n",
    "\n",
    "        # CNN Encoder with more depth and dropout\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout_cnn = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Flattened size for input image 9x9 → (128, 3, 3)\n",
    "        conv_output_size = 128 * 3 * 3\n",
    "        embed_size = 16 + 8 + 12\n",
    "\n",
    "        # Latent space\n",
    "        self.fc1 = nn.Linear(conv_output_size + embed_size, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim + embed_size, 128)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 64)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
    "        self.decoder_out = nn.Linear(64, 1)\n",
    "\n",
    "    def encode(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Encodes the input image and metadata into latent mean and log-variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices of shape (B,).\n",
    "        time_idx : Tensor\n",
    "            Time class indices of shape (B,).\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices of shape (B,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu : Tensor\n",
    "            Mean of the latent Gaussian distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.bn1(self.conv1(img)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = self.dropout_cnn(h)\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "\n",
    "        # Embeddings\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "        emb = torch.cat([v_emb, t_emb, a_emb], dim=1)\n",
    "\n",
    "        h_combined = torch.cat([h, emb], dim=1)\n",
    "        h1 = F.relu(self.bn_fc1(self.fc1(h_combined)))\n",
    "        h1 = self.dropout_fc(h1)\n",
    "\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) using standard normal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : Tensor\n",
    "            Mean of the latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Sampled latent vector z.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector and metadata into the predicted SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : Tensor\n",
    "            Latent vector of shape (B, latent_dim).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        \"\"\"\n",
    "        v_emb = self.voltage_embed(voltage_idx)\n",
    "        t_emb = self.time_embed(time_idx)\n",
    "        a_emb = self.agent_embed(agent_idx)\n",
    "\n",
    "        # Adding Noise for Robustness Check\n",
    "        v_emb = v_emb + torch.randn_like(v_emb) * 0.5\n",
    "        t_emb = t_emb + torch.randn_like(t_emb) * 0.5\n",
    "        a_emb = a_emb + torch.randn_like(a_emb) * 0.5\n",
    "        \n",
    "        emb = torch.cat([v_emb, t_emb, a_emb], dim=1)\n",
    "\n",
    "        z_combined = torch.cat([z, emb], dim=1)\n",
    "        h = F.relu(self.bn_dec1(self.decoder_fc1(z_combined)))\n",
    "        h = F.relu(self.bn_dec2(self.decoder_fc2(h)))\n",
    "        snr_pred = self.decoder_out(h)\n",
    "        return snr_pred.squeeze(1)\n",
    "\n",
    "    def forward(self, img, voltage_idx, time_idx, agent_idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes input, samples from latent space, and decodes to predict SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : Tensor\n",
    "            Input image tensor of shape (B, 1, H, W).\n",
    "        voltage_idx : Tensor\n",
    "            Voltage class indices.\n",
    "        time_idx : Tensor\n",
    "            Time class indices.\n",
    "        agent_idx : Tensor\n",
    "            Contrast agent class indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snr_pred : Tensor\n",
    "            Predicted SNR values of shape (B,).\n",
    "        mu : Tensor\n",
    "            Mean of latent distribution.\n",
    "        logvar : Tensor\n",
    "            Log-variance of latent distribution.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(img, voltage_idx, time_idx, agent_idx)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        snr_pred = self.decode(z, voltage_idx, time_idx, agent_idx)\n",
    "        return snr_pred, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# get_data_loaders function:\n",
    "# Loads a CT dataset with images and metadata.\n",
    "# Applies quantile binning to stratify based on SNR values.\n",
    "# Splits the dataset into training and validation sets.\n",
    "# Augments extreme SNR samples to better handle edge cases.\n",
    "# Applies image transformations.\n",
    "# Balances the training set using weighted sampling to address SNR distribution imbalance.\n",
    "############################################################################################\n",
    "\n",
    "def get_data_loaders(seed, batch_size=16, n_bins=30, extreme_percentile=5):\n",
    "    \"\"\"\n",
    "    Prepares stratified, augmented, and balanced DataLoaders for training and validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    batch_size : int, optional\n",
    "        Batch size for both training and validation DataLoaders. Default is 16.\n",
    "    \n",
    "    n_bins : int, optional\n",
    "        Number of quantile bins to use for stratified splitting and weighted sampling. Default is 30.\n",
    "    \n",
    "    extreme_percentile : float, optional\n",
    "        Percentile threshold to define \"extreme\" SNR values (low and high ends). Default is 5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loader : DataLoader\n",
    "        PyTorch DataLoader for the training set with data augmentation and weighted sampling.\n",
    "    \n",
    "    val_loader : DataLoader\n",
    "        PyTorch DataLoader for the validation set with deterministic sampling and no augmentation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # Transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((9, 9)),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((9, 9)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Load full dataset\n",
    "    base_path = Path(\"../dataset\")\n",
    "\n",
    "    full_dataset = CapriCTDataset(\n",
    "        metadata_csv= base_path / \"final_dataset.csv\",\n",
    "        img_folder_path= base_path / \"img\" ,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    # Get all SNR values\n",
    "    all_snr = np.array([full_dataset[i][4] for i in range(len(full_dataset))])\n",
    "\n",
    "    # Quantile binning for stratification\n",
    "    snr_bins = pd.qcut(all_snr, q=n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "    # Train/Val split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(full_dataset)),\n",
    "        test_size=0.2,\n",
    "        random_state=seed,\n",
    "        stratify=snr_bins\n",
    "    )\n",
    "\n",
    "    # Build subsets\n",
    "    class TransformedSubset(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img, voltage, time, agent, snr = self.subset[idx]\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, voltage, time, agent, snr\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "\n",
    "    train_subset = Subset(full_dataset, train_indices)\n",
    "    val_subset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    val_dataset = TransformedSubset(val_subset, val_transform)\n",
    "\n",
    "    # --- Duplicate extreme values dynamically ---\n",
    "    train_snr = np.array([full_dataset[i][4] for i in train_indices])\n",
    "    lower_thresh = np.percentile(train_snr, extreme_percentile)\n",
    "    upper_thresh = np.percentile(train_snr, 100 - extreme_percentile)\n",
    "\n",
    "    # Identify extreme samples\n",
    "    extreme_indices = [i for i in train_indices if full_dataset[i][4] < lower_thresh or full_dataset[i][4] > upper_thresh]\n",
    "\n",
    "    duplicated_extremes = Subset(full_dataset, extreme_indices)\n",
    "    duplicated_extremes = TransformedSubset(duplicated_extremes, train_transform)\n",
    "\n",
    "    # Wrap the base train set with transform\n",
    "    train_dataset_base = TransformedSubset(train_subset, train_transform)\n",
    "\n",
    "    # Combine datasets: original + duplicated extremes\n",
    "    combined_train_dataset = ConcatDataset([train_dataset_base,\n",
    "                                            duplicated_extremes\n",
    "                                            ] )\n",
    "\n",
    "    # --- Weighted Sampling ---\n",
    "    combined_snr = np.array([combined_train_dataset[i][4] for i in range(len(combined_train_dataset))])\n",
    "    combined_bins = pd.qcut(combined_snr, q=n_bins, labels=False, duplicates='drop')\n",
    "    bin_counts = np.bincount(combined_bins)\n",
    "    bin_weights = 1.0 / bin_counts\n",
    "    weights = [bin_weights[b] for b in combined_bins]\n",
    "\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(combined_train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Setting the seed value for each training loop\n",
    "###############################################################\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed across Python, NumPy, and PyTorch (CPU and GPU) for reproducibility.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The seed value to ensure deterministic behavior across runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# This method calculates the vae LOSS with the preds and targets provided\n",
    "#############################################################################\n",
    "\n",
    "def vae_loss(snr_pred, snr_true, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the total loss for a Variational Autoencoder (VAE), combining \n",
    "    reconstruction loss and KL divergence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    snr_pred : Tensor\n",
    "        Predicted SNR values from the decoder. Shape: (B,)\n",
    "    \n",
    "    snr_true : Tensor\n",
    "        Ground-truth SNR values. Shape: (B,)\n",
    "    \n",
    "    mu : Tensor\n",
    "        Latent mean vector from the encoder. Shape: (B, latent_dim)\n",
    "    \n",
    "    logvar : Tensor\n",
    "        Log-variance vector from the encoder. Shape: (B, latent_dim)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_loss : Tensor\n",
    "        Sum of reconstruction loss and KL divergence.\n",
    "    \n",
    "    recon_loss : Tensor\n",
    "        Mean squared error (MSE) between predicted and true SNR values.\n",
    "    \n",
    "    kld : Tensor\n",
    "        KL divergence between the learned latent distribution and standard normal.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - KL divergence is scaled by the batch size to ensure stability across varying batch sizes.\n",
    "    - This function assumes `snr_pred` and `snr_true` are both 1D tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    recon_loss = F.mse_loss(snr_pred, snr_true, reduction='mean')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / snr_true.size(0)\n",
    "    return recon_loss + kld, recon_loss, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030eac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Below validate model method calculates the model performance against \n",
    "# validation dataset and returns the metrics \n",
    "##########################################################################\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the VAE model on a validation set and computes loss and performance metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained VAE model to be evaluated.\n",
    "    \n",
    "    val_loader : DataLoader\n",
    "        PyTorch DataLoader containing the validation dataset.\n",
    "    \n",
    "    device : torch.device\n",
    "        The device on which computation will be performed (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    val_loss : float\n",
    "        Average Smooth L1 loss over the entire validation set.\n",
    "    \n",
    "    r2 : float\n",
    "        Coefficient of determination (R² score) between predicted and true SNR values.\n",
    "    \n",
    "    rmse : float\n",
    "        Root Mean Squared Error (RMSE) between predicted and true SNR values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The model is set to evaluation mode during validation (`model.eval()`).\n",
    "    - No gradient computation is performed (`torch.no_grad()` context).\n",
    "    - `Smooth L1 loss` is used as a robust regression loss function.\n",
    "    - Predictions and ground truths are collected across all batches to compute R² and RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, voltage, time, agent_idx, snr_true in val_loader:\n",
    "            image = image.to(device)\n",
    "            voltage = voltage.to(device)\n",
    "            time = time.to(device)\n",
    "            agent_idx = agent_idx.to(device)\n",
    "            snr_true = snr_true.to(device)\n",
    "\n",
    "            snr_pred, mu, logvar = model(image, voltage, time, agent_idx)\n",
    "\n",
    "            # Use consistent loss (Smooth L1 or any custom one)\n",
    "            loss = F.smooth_l1_loss(snr_pred.squeeze(), snr_true.squeeze())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(snr_pred.squeeze().cpu().numpy())\n",
    "            all_targets.extend(snr_true.squeeze().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    rmse = mean_squared_error(all_targets, all_preds)\n",
    "\n",
    "    return val_loss, r2, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# This method trains a single causal model in Ensemble modelling by calculating the training and \n",
    "# validation losses implemented with the Early stopping method \n",
    "# if the validation loss starts increasing\n",
    "#################################################################################################\n",
    "\n",
    "def train_vae_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs=100):\n",
    "    \"\"\"\n",
    "    Trains a Variational Autoencoder (VAE) model using a custom loss function (reconstruction + KL divergence),\n",
    "    and evaluates it on a validation set after each epoch.\n",
    "\n",
    "    Implements early stopping based on validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The VAE model to be trained.\n",
    "\n",
    "    train_loader : DataLoader\n",
    "        PyTorch DataLoader for the training dataset.\n",
    "\n",
    "    val_loader : DataLoader\n",
    "        PyTorch DataLoader for the validation dataset.\n",
    "\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for training (e.g., Adam).\n",
    "\n",
    "    scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        Learning rate scheduler to step using validation loss.\n",
    "\n",
    "    device : torch.device\n",
    "        Device to run training on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    epochs : int, optional\n",
    "        Maximum number of training epochs. Default is 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module\n",
    "        The trained model with parameters from the best validation performance.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses a combined loss: MSE (reconstruction) + KL divergence.\n",
    "    - Gradients are clipped to `max_norm=1.0` to stabilize training.\n",
    "    - Early stopping is triggered after `patience` epochs of no validation improvement.\n",
    "    - Validation metrics include R² score and RMSE, in addition to loss.\n",
    "    - Logs training and validation metrics for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_recon, total_kl = 0.0, 0.0, 0.0\n",
    "\n",
    "        for image, voltage, time, agent_idx, snr in train_loader:\n",
    "            image = image.to(device)\n",
    "            voltage = voltage.to(device)\n",
    "            time = time.to(device)\n",
    "            agent_idx = agent_idx.to(device)\n",
    "            snr_true = snr.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            snr_pred, mu, logvar = model(image, voltage, time, agent_idx)\n",
    "\n",
    "            loss, recon_loss, kl_loss = vae_loss(snr_pred, snr_true, mu, logvar)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            total_kl += kl_loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_loss, val_r2, val_rmse = validate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f} | R²: {val_r2:.4f} | RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# The below method trains the ensemble models together\n",
    "# it calls the train_single_model method with given parameters\n",
    "#############################################################################\n",
    "\n",
    "def train_ensemble(num_models=5, seeds=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Trains an ensemble of VAE models with different random seeds for improved robustness and generalization.\n",
    "\n",
    "    Each model in the ensemble is:\n",
    "    - Initialized with a different seed.\n",
    "    - Trained independently using a unique data split (stratified by SNR).\n",
    "    - Appended to the returned list for later ensembling or evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_models : int, optional\n",
    "        Number of models to train in the ensemble. Default is 5.\n",
    "    \n",
    "    seeds : list of int, optional\n",
    "        List of seeds to use for each model. If None, generates seeds starting from 42 with a step of 10.\n",
    "    \n",
    "    device : str or torch.device, optional\n",
    "        Device on which to train the models (e.g., 'cpu' or 'cuda'). Default is 'cpu'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ensemble_models : list of torch.nn.Module\n",
    "        List of trained VAE models, each trained with a different seed and dataset split.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses `train_vae_model()` for training individual models.\n",
    "    - Applies `set_seed()` for reproducibility across data splits and weight initialization.\n",
    "    - Each model uses a fresh instance of `CapriCTCausalVAEModel` with `latent_dim=256`.\n",
    "    - Uses AdamW optimizer and ReduceLROnPlateau scheduler for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    ensemble_models = []\n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [42 + i * 10 for i in range(num_models)]\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\nTraining model {i+1}/{num_models} with seed {seed}\")\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Load data for the given seed\n",
    "        train_loader, val_loader = get_data_loaders(seed, batch_size=16)\n",
    "\n",
    "        # Initialize the model\n",
    "        # model = CapriCTWithoutAgentModel(latent_dim=256).to(device)\n",
    "        # model = CapriCTWithoutVoltageModel(latent_dim=256).to(device)\n",
    "        # model = CapriCTWithoutTimeModel(latent_dim=256).to(device)\n",
    "        # model = CapriCTWithoutEmbedModel(latent_dim=256).to(device)\n",
    "        model =CapriCTRobustCheckModel(latent_dim=256).to(device)\n",
    "\n",
    "        # Optimizer and learning rate scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "        # Train the model\n",
    "        trained_model = train_vae_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=100\n",
    "        )\n",
    "\n",
    "        ensemble_models.append(trained_model)\n",
    "\n",
    "    return ensemble_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b03d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# This method uses the trained ensemble models to predict SNR values \n",
    "# along with the respective targets\n",
    "#############################################################################\n",
    "\n",
    "def get_ensemble_predictions(models, val_loader, device):\n",
    "    \"\"\"\n",
    "    Generates predictions from an ensemble of trained models on a validation set.\n",
    "\n",
    "    Each model makes independent predictions on the same validation data,\n",
    "    and all predictions are collected for later aggregation (e.g., mean, median).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list of torch.nn.Module\n",
    "        List of trained VAE models forming the ensemble.\n",
    "    \n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    \n",
    "    device : str or torch.device\n",
    "        Device to use for inference (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_preds : list of list of float\n",
    "        A list containing per-model predictions. Each inner list has predictions of one model \n",
    "        for the full validation set.\n",
    "    \n",
    "    targets : list of float\n",
    "        Ground-truth SNR values from the validation set (shared across all models).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Assumes all models are compatible with the same input format and produce scalar SNR predictions.\n",
    "    - Targets are extracted only once from the first pass through the loader.\n",
    "    - Output can be post-processed (e.g., averaged) to get the final ensemble prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_preds = []\n",
    "    targets = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, voltage_idx, time_idx, agent_idx, snr in val_loader:\n",
    "                image = image.to(device)\n",
    "                voltage_idx = voltage_idx.to(device).long()\n",
    "                time_idx = time_idx.to(device).long()\n",
    "                agent_idx = agent_idx.to(device).long()\n",
    "\n",
    "                snr_pred, _, _ = model(image, voltage_idx, time_idx, agent_idx)\n",
    "                model_preds.extend(snr_pred.squeeze().cpu().numpy())\n",
    "\n",
    "                # Store targets only once\n",
    "                if len(targets) < len(val_loader.dataset):\n",
    "                    targets.extend(snr.squeeze().cpu().numpy())\n",
    "\n",
    "        all_preds.append(model_preds)\n",
    "\n",
    "    return all_preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# This method evaluates the predicted SNR values with the targets values\n",
    "#################################################################################\n",
    "\n",
    "def evaluate_models(all_preds, targets):\n",
    "    \"\"\"\n",
    "    Evaluate individual model predictions and the ensemble.\n",
    "\n",
    "    Args:\n",
    "        all_preds (list or np.ndarray or torch.Tensor): Shape [num_models, num_samples]\n",
    "        targets (list or np.ndarray or torch.Tensor): Shape [num_samples]\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'individual_metrics': List[dict],\n",
    "            'best_model_index': int,\n",
    "            'ensemble_metrics': dict\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Convert inputs to tensors\n",
    "    if isinstance(all_preds, np.ndarray):\n",
    "        all_preds = torch.from_numpy(all_preds)\n",
    "    elif isinstance(all_preds, list):\n",
    "        all_preds = torch.stack([torch.tensor(p) for p in all_preds])\n",
    "    elif not isinstance(all_preds, torch.Tensor):\n",
    "        raise TypeError(\"all_preds must be a list, np.ndarray, or torch.Tensor\")\n",
    "\n",
    "    if isinstance(targets, np.ndarray):\n",
    "        targets = torch.from_numpy(targets)\n",
    "    elif isinstance(targets, list):\n",
    "        targets = torch.tensor(targets)\n",
    "    elif not isinstance(targets, torch.Tensor):\n",
    "        raise TypeError(\"targets must be a list, np.ndarray, or torch.Tensor\")\n",
    "\n",
    "    targets = targets.squeeze()\n",
    "    num_models = all_preds.shape[0]\n",
    "\n",
    "    individual_metrics = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model_idx = -1\n",
    "\n",
    "    # Evaluate each model\n",
    "    for i in range(num_models):\n",
    "        preds = all_preds[i].squeeze().cpu().numpy()\n",
    "        targs = targets.cpu().numpy()\n",
    "\n",
    "        r2 = r2_score(targs, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(targs, preds))\n",
    "        mae = mean_absolute_error(targs, preds)\n",
    "\n",
    "        individual_metrics.append({'model_idx': i, 'r2': r2, 'rmse': rmse, 'mae': mae})\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model_idx = i\n",
    "\n",
    "    # Ensemble (mean of all predictions)\n",
    "    mean_preds = all_preds.mean(dim=0).squeeze().cpu().numpy()\n",
    "    std_preds = all_preds.std(dim=0, unbiased=False).squeeze().cpu().numpy()\n",
    "    targets_np = targets.cpu().numpy()\n",
    "\n",
    "    r2_ens = r2_score(targets_np, mean_preds)\n",
    "    rmse_ens = np.sqrt(mean_squared_error(targets_np, mean_preds))\n",
    "    mae_ens = mean_absolute_error(targets_np, mean_preds)\n",
    "\n",
    "    ensemble_metrics = {\n",
    "        'r2': r2_ens,\n",
    "        'rmse': rmse_ens,\n",
    "        'mae': mae_ens,\n",
    "        'mean_preds': mean_preds,\n",
    "        'std_preds': std_preds,\n",
    "        'targets': targets_np\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'individual_metrics': individual_metrics,\n",
    "        'best_model_index': best_model_idx,\n",
    "        'ensemble_metrics': ensemble_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ce286",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Initializing the dataset for Intervention and counterfactual inference\n",
    "###########################################################################\n",
    "\n",
    "def get_dataset():\n",
    "    \"\"\"\n",
    "    Loads and returns the CapriCTDataset with predefined image transformations.\n",
    "\n",
    "    Applies a consistent transform to resize CT images and convert them to tensors.\n",
    "    Assumes the dataset CSV and image directory are located under a relative `../dataset/` path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : CapriCTDataset\n",
    "        An instance of the custom dataset with image and metadata fields prepared\n",
    "        for model training or inference.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Images are resized to 9x9 pixels and converted to tensors.\n",
    "    - Paths:\n",
    "        - Metadata CSV: ../dataset/final_dataset.csv\n",
    "        - Image folder: ../dataset/img/\n",
    "    \"\"\"\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((9, 9)),  \n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "    base_path = Path(\"../dataset\")\n",
    "\n",
    "    dataset = CapriCTDataset(\n",
    "        metadata_csv= base_path / \"final_dataset.csv\",\n",
    "        img_folder_path= base_path / \"img\" ,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c82fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/5 with seed 42\n",
      "Epoch 1/100 | Train Loss: 79485.0954 | Val Loss: 150.6637 | R²: -0.1132 | RMSE: 62937.3320\n",
      "Epoch 2/100 | Train Loss: 84095.8979 | Val Loss: 150.8500 | R²: -0.0994 | RMSE: 62152.8242\n",
      "Epoch 3/100 | Train Loss: 83578.2689 | Val Loss: 150.0239 | R²: -0.1035 | RMSE: 62385.9492\n",
      "Epoch 4/100 | Train Loss: 83358.7015 | Val Loss: 150.8885 | R²: -0.0901 | RMSE: 61629.2734\n",
      "Epoch 5/100 | Train Loss: 81290.0173 | Val Loss: 153.6316 | R²: -0.0568 | RMSE: 59746.0195\n",
      "Epoch 6/100 | Train Loss: 87824.4175 | Val Loss: 153.0011 | R²: -0.0606 | RMSE: 59961.5078\n",
      "Epoch 7/100 | Train Loss: 76196.3481 | Val Loss: 156.7072 | R²: -0.0294 | RMSE: 58195.1406\n",
      "Epoch 8/100 | Train Loss: 78213.4565 | Val Loss: 159.3780 | R²: -0.0275 | RMSE: 58088.1172\n",
      "Epoch 9/100 | Train Loss: 75934.6944 | Val Loss: 159.1071 | R²: -0.0044 | RMSE: 56784.6797\n",
      "Epoch 10/100 | Train Loss: 79389.4736 | Val Loss: 162.9792 | R²: 0.0025 | RMSE: 56392.2188\n",
      "Epoch 11/100 | Train Loss: 72835.8719 | Val Loss: 159.9007 | R²: -0.0127 | RMSE: 57254.8516\n",
      "Epoch 12/100 | Train Loss: 77342.2035 | Val Loss: 164.5164 | R²: 0.0203 | RMSE: 55387.3047\n",
      "Epoch 13/100 | Train Loss: 76921.9792 | Val Loss: 162.4152 | R²: 0.0096 | RMSE: 55992.5664\n",
      "Early stopping at epoch 13\n",
      "\n",
      "Training model 2/5 with seed 52\n",
      "Epoch 1/100 | Train Loss: 87483.4502 | Val Loss: 151.6849 | R²: -0.1222 | RMSE: 63539.5781\n",
      "Epoch 2/100 | Train Loss: 87031.7678 | Val Loss: 152.1517 | R²: -0.0983 | RMSE: 62184.0273\n",
      "Epoch 3/100 | Train Loss: 86323.5638 | Val Loss: 151.8334 | R²: -0.0968 | RMSE: 62103.7109\n",
      "Epoch 4/100 | Train Loss: 86729.8953 | Val Loss: 151.7896 | R²: -0.0948 | RMSE: 61988.1250\n",
      "Epoch 5/100 | Train Loss: 85384.2278 | Val Loss: 154.4993 | R²: -0.0416 | RMSE: 58974.8516\n",
      "Epoch 6/100 | Train Loss: 79723.8433 | Val Loss: 155.7187 | R²: -0.0471 | RMSE: 59289.2969\n",
      "Epoch 7/100 | Train Loss: 82527.4100 | Val Loss: 160.5418 | R²: -0.0009 | RMSE: 56673.7695\n",
      "Epoch 8/100 | Train Loss: 79935.5052 | Val Loss: 161.0402 | R²: -0.0043 | RMSE: 56862.4023\n",
      "Epoch 9/100 | Train Loss: 79692.1079 | Val Loss: 166.6710 | R²: 0.0073 | RMSE: 56206.6406\n",
      "Epoch 10/100 | Train Loss: 78248.0733 | Val Loss: 162.7607 | R²: 0.0072 | RMSE: 56214.3828\n",
      "Epoch 11/100 | Train Loss: 76056.7637 | Val Loss: 165.1580 | R²: -0.0156 | RMSE: 57501.6523\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training model 3/5 with seed 62\n",
      "Epoch 1/100 | Train Loss: 89323.8205 | Val Loss: 150.4975 | R²: -0.1199 | RMSE: 62864.9570\n",
      "Epoch 2/100 | Train Loss: 93634.7100 | Val Loss: 150.8883 | R²: -0.0968 | RMSE: 61567.1953\n",
      "Epoch 3/100 | Train Loss: 85415.8574 | Val Loss: 151.9131 | R²: -0.0534 | RMSE: 59134.6523\n",
      "Epoch 4/100 | Train Loss: 87344.6607 | Val Loss: 150.2542 | R²: -0.0737 | RMSE: 60272.6953\n",
      "Epoch 5/100 | Train Loss: 80055.1083 | Val Loss: 149.0772 | R²: -0.0594 | RMSE: 59470.6172\n",
      "Epoch 6/100 | Train Loss: 82555.9761 | Val Loss: 155.2432 | R²: 0.0039 | RMSE: 55917.2695\n",
      "Epoch 7/100 | Train Loss: 82455.8518 | Val Loss: 158.3130 | R²: 0.0004 | RMSE: 56109.7695\n",
      "Epoch 8/100 | Train Loss: 79838.5508 | Val Loss: 160.4220 | R²: -0.0366 | RMSE: 58188.6016\n",
      "Epoch 9/100 | Train Loss: 82508.3505 | Val Loss: 252.6618 | R²: -36.0783 | RMSE: 2081376.2500\n",
      "Epoch 10/100 | Train Loss: 76957.3977 | Val Loss: 159.8420 | R²: 0.0047 | RMSE: 55872.4375\n",
      "Epoch 11/100 | Train Loss: 76404.8617 | Val Loss: 162.2243 | R²: 0.0202 | RMSE: 55002.4141\n",
      "Epoch 12/100 | Train Loss: 79798.1220 | Val Loss: 164.0131 | R²: 0.0589 | RMSE: 52826.3633\n",
      "Epoch 13/100 | Train Loss: 80356.3604 | Val Loss: 178.9024 | R²: -0.2598 | RMSE: 70716.7656\n",
      "Epoch 14/100 | Train Loss: 75888.7531 | Val Loss: 167.2992 | R²: 0.0556 | RMSE: 53016.1914\n",
      "Epoch 15/100 | Train Loss: 79037.7839 | Val Loss: 170.7916 | R²: -0.1193 | RMSE: 62833.2891\n",
      "Early stopping at epoch 15\n",
      "\n",
      "Training model 4/5 with seed 72\n",
      "Epoch 1/100 | Train Loss: 84553.0078 | Val Loss: 150.1642 | R²: -0.1209 | RMSE: 63016.2461\n",
      "Epoch 2/100 | Train Loss: 86195.5009 | Val Loss: 150.7906 | R²: -0.1162 | RMSE: 62751.7539\n",
      "Epoch 3/100 | Train Loss: 80117.9352 | Val Loss: 151.0258 | R²: -0.0844 | RMSE: 60964.5078\n",
      "Epoch 4/100 | Train Loss: 83404.7137 | Val Loss: 155.1474 | R²: -0.0970 | RMSE: 61671.5234\n",
      "Epoch 5/100 | Train Loss: 79589.1471 | Val Loss: 156.9266 | R²: -0.0468 | RMSE: 58853.2734\n",
      "Epoch 6/100 | Train Loss: 73527.5211 | Val Loss: 156.9724 | R²: -0.0135 | RMSE: 56978.4414\n",
      "Epoch 7/100 | Train Loss: 79714.7819 | Val Loss: 158.8846 | R²: -0.0353 | RMSE: 58202.3672\n",
      "Epoch 8/100 | Train Loss: 78293.6224 | Val Loss: 163.3696 | R²: -0.0057 | RMSE: 56537.7031\n",
      "Epoch 9/100 | Train Loss: 78688.6267 | Val Loss: 160.7442 | R²: -0.0312 | RMSE: 57970.9141\n",
      "Epoch 10/100 | Train Loss: 76049.5813 | Val Loss: 157.5624 | R²: -0.0441 | RMSE: 58699.2266\n",
      "Epoch 11/100 | Train Loss: 74536.4206 | Val Loss: 163.5350 | R²: -0.0206 | RMSE: 57379.0586\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training model 5/5 with seed 82\n",
      "Epoch 1/100 | Train Loss: 88913.3811 | Val Loss: 151.4741 | R²: -0.1357 | RMSE: 64184.8906\n",
      "Epoch 2/100 | Train Loss: 91365.3829 | Val Loss: 152.0251 | R²: -0.1169 | RMSE: 63124.0156\n",
      "Epoch 3/100 | Train Loss: 86813.5890 | Val Loss: 153.7096 | R²: -0.0816 | RMSE: 61128.6250\n",
      "Epoch 4/100 | Train Loss: 86007.2344 | Val Loss: 153.2593 | R²: -0.0848 | RMSE: 61310.3398\n",
      "Epoch 5/100 | Train Loss: 87025.4270 | Val Loss: 177.9174 | R²: -1.6636 | RMSE: 150535.6719\n",
      "Epoch 6/100 | Train Loss: 82118.0961 | Val Loss: 157.7148 | R²: -0.0313 | RMSE: 58285.7148\n",
      "Epoch 7/100 | Train Loss: 82436.3739 | Val Loss: 157.4842 | R²: 0.0024 | RMSE: 56382.6992\n",
      "Epoch 8/100 | Train Loss: 80932.1087 | Val Loss: 164.3876 | R²: -0.2039 | RMSE: 68037.8281\n",
      "Epoch 9/100 | Train Loss: 80030.6127 | Val Loss: 1282.6307 | R²: -7354.2518 | RMSE: 415688576.0000\n",
      "Epoch 10/100 | Train Loss: 80489.0390 | Val Loss: 172.9566 | R²: 0.0385 | RMSE: 54341.2617\n",
      "Epoch 11/100 | Train Loss: 75865.5631 | Val Loss: 163.1399 | R²: 0.0221 | RMSE: 55266.1406\n",
      "Early stopping at epoch 11\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Setting the device parameter\n",
    "##########################################\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "##########################################\n",
    "# Train ensemble models without Agent\n",
    "##########################################\n",
    "models = train_ensemble(num_models=5, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4122384",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Get predictions and targets\n",
    "################################################\n",
    "\n",
    "_, val_loader = get_data_loaders(seed=42, batch_size=16)\n",
    "all_preds, targets = get_ensemble_predictions(models, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Individual Model Metrics********************************\n",
      "Model 1 → R2: 0.007, RMSE: 236.926, MAE: 164.079\n",
      "Model 2 → R2: -0.486, RMSE: 289.824, MAE: 170.957\n",
      "Model 3 → R2: -0.019, RMSE: 240.078, MAE: 165.522\n",
      "Model 4 → R2: 0.021, RMSE: 235.202, MAE: 160.408\n",
      "Model 5 → R2: 0.032, RMSE: 233.931, MAE: 162.385\n",
      "***********************************************************************\n",
      "\n",
      "Best Model : Model 5\n",
      "***********************************************************************\n",
      "Ensemble R2: 0.005\n",
      "Ensemble RMSE: 237.137\n",
      "Ensemble MAE: 164.256\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Evaluate the ensemble models for our CAPRI-CT without agent\n",
    "#####################################################################\n",
    "\n",
    "result = evaluate_models(all_preds, targets)\n",
    "\n",
    "# View metrics\n",
    "print(f\"***************Individual Model Metrics********************************\")\n",
    "for m in result['individual_metrics']:\n",
    "    print(f\"Model {m['model_idx']+1} → R2: {m['r2']:.3f}, RMSE: {m['rmse']:.3f}, MAE: {m['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n",
    "\n",
    "print(f\"\\nBest Model : Model {result['best_model_index']+1}\")\n",
    "print(f\"***********************************************************************\")\n",
    "print(f\"Ensemble R2: {result['ensemble_metrics']['r2']:.3f}\")\n",
    "print(f\"Ensemble RMSE: {result['ensemble_metrics']['rmse']:.3f}\")\n",
    "print(f\"Ensemble MAE: {result['ensemble_metrics']['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25167fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/5 with seed 42\n",
      "Epoch 1/100 | Train Loss: 81786.7895 | Val Loss: 145.3927 | R²: -0.0640 | RMSE: 60153.8086\n",
      "Epoch 2/100 | Train Loss: 81699.3956 | Val Loss: 143.5181 | R²: -0.0409 | RMSE: 58848.9844\n",
      "Epoch 3/100 | Train Loss: 74642.0978 | Val Loss: 135.1902 | R²: 0.0794 | RMSE: 52046.7891\n",
      "Epoch 4/100 | Train Loss: 71125.3283 | Val Loss: 132.1728 | R²: 0.1454 | RMSE: 48316.2891\n",
      "Epoch 5/100 | Train Loss: 65198.5932 | Val Loss: 125.8901 | R²: 0.2104 | RMSE: 44638.6445\n",
      "Epoch 6/100 | Train Loss: 62667.3133 | Val Loss: 138.2948 | R²: 0.1105 | RMSE: 50285.5273\n",
      "Epoch 7/100 | Train Loss: 49364.3505 | Val Loss: 115.1842 | R²: 0.3554 | RMSE: 36441.8047\n",
      "Epoch 8/100 | Train Loss: 46859.2113 | Val Loss: 116.6540 | R²: 0.3783 | RMSE: 35147.9727\n",
      "Epoch 9/100 | Train Loss: 40591.7773 | Val Loss: 135.6205 | R²: 0.2360 | RMSE: 43193.2891\n",
      "Epoch 10/100 | Train Loss: 38984.4558 | Val Loss: 104.2064 | R²: 0.5259 | RMSE: 26803.6270\n",
      "Epoch 11/100 | Train Loss: 34568.9005 | Val Loss: 120.8455 | R²: 0.4911 | RMSE: 28768.9707\n",
      "Epoch 12/100 | Train Loss: 33775.5542 | Val Loss: 136.1572 | R²: 0.3138 | RMSE: 38792.4258\n",
      "Epoch 13/100 | Train Loss: 31872.0447 | Val Loss: 97.7761 | R²: 0.6277 | RMSE: 21047.3008\n",
      "Epoch 14/100 | Train Loss: 28817.5052 | Val Loss: 93.0207 | R²: 0.6454 | RMSE: 20046.9531\n",
      "Epoch 15/100 | Train Loss: 26930.1877 | Val Loss: 121.8729 | R²: 0.2845 | RMSE: 40451.1602\n",
      "Epoch 16/100 | Train Loss: 28820.7702 | Val Loss: 111.8511 | R²: 0.4942 | RMSE: 28596.6719\n",
      "Epoch 17/100 | Train Loss: 29025.5501 | Val Loss: 95.8263 | R²: 0.6450 | RMSE: 20068.7207\n",
      "Epoch 18/100 | Train Loss: 28839.5912 | Val Loss: 110.6949 | R²: 0.5247 | RMSE: 26869.9785\n",
      "Epoch 19/100 | Train Loss: 26190.3507 | Val Loss: 99.6440 | R²: 0.6385 | RMSE: 20437.7324\n",
      "Epoch 20/100 | Train Loss: 27192.8172 | Val Loss: 109.5440 | R²: 0.5241 | RMSE: 26904.0938\n",
      "Epoch 21/100 | Train Loss: 26658.0574 | Val Loss: 93.1726 | R²: 0.6538 | RMSE: 19571.6191\n",
      "Epoch 22/100 | Train Loss: 26193.7121 | Val Loss: 94.6644 | R²: 0.6447 | RMSE: 20084.9121\n",
      "Epoch 23/100 | Train Loss: 27511.5488 | Val Loss: 92.0333 | R²: 0.6553 | RMSE: 19488.5586\n",
      "Epoch 24/100 | Train Loss: 27591.7986 | Val Loss: 92.4525 | R²: 0.6502 | RMSE: 19774.0566\n",
      "Epoch 25/100 | Train Loss: 27736.8090 | Val Loss: 92.5841 | R²: 0.6516 | RMSE: 19695.9844\n",
      "Epoch 26/100 | Train Loss: 27272.1309 | Val Loss: 103.9984 | R²: 0.6285 | RMSE: 21002.2461\n",
      "Epoch 27/100 | Train Loss: 26594.3963 | Val Loss: 93.9172 | R²: 0.6402 | RMSE: 20341.9375\n",
      "Epoch 28/100 | Train Loss: 28192.0265 | Val Loss: 114.2935 | R²: 0.4884 | RMSE: 28926.2383\n",
      "Epoch 29/100 | Train Loss: 26001.3617 | Val Loss: 94.7480 | R²: 0.6497 | RMSE: 19803.5625\n",
      "Epoch 30/100 | Train Loss: 27196.6451 | Val Loss: 94.6454 | R²: 0.6363 | RMSE: 20562.2617\n",
      "Epoch 31/100 | Train Loss: 25434.3367 | Val Loss: 93.0495 | R²: 0.6436 | RMSE: 20149.7930\n",
      "Epoch 32/100 | Train Loss: 26311.7015 | Val Loss: 96.4459 | R²: 0.6091 | RMSE: 22101.3457\n",
      "Epoch 33/100 | Train Loss: 25166.0166 | Val Loss: 93.7273 | R²: 0.6559 | RMSE: 19455.9043\n",
      "Early stopping at epoch 33\n",
      "\n",
      "Training model 2/5 with seed 52\n",
      "Epoch 1/100 | Train Loss: 87265.3421 | Val Loss: 147.6983 | R²: -0.0828 | RMSE: 61305.9648\n",
      "Epoch 2/100 | Train Loss: 83989.6019 | Val Loss: 144.6705 | R²: -0.0349 | RMSE: 58597.9766\n",
      "Epoch 3/100 | Train Loss: 74440.1421 | Val Loss: 137.7264 | R²: 0.0707 | RMSE: 52616.1445\n",
      "Epoch 4/100 | Train Loss: 71400.9942 | Val Loss: 134.1974 | R²: 0.1294 | RMSE: 49296.4375\n",
      "Epoch 5/100 | Train Loss: 67151.4092 | Val Loss: 129.4432 | R²: 0.1967 | RMSE: 45481.4219\n",
      "Epoch 6/100 | Train Loss: 57706.9401 | Val Loss: 122.4241 | R²: 0.3171 | RMSE: 38665.8594\n",
      "Epoch 7/100 | Train Loss: 53055.5613 | Val Loss: 115.4732 | R²: 0.4084 | RMSE: 33494.0742\n",
      "Epoch 8/100 | Train Loss: 47361.8652 | Val Loss: 113.1302 | R²: 0.4523 | RMSE: 31010.7246\n",
      "Epoch 9/100 | Train Loss: 41468.0267 | Val Loss: 137.2810 | R²: 0.3045 | RMSE: 39376.8828\n",
      "Epoch 10/100 | Train Loss: 34968.0191 | Val Loss: 114.2856 | R²: 0.4962 | RMSE: 28524.5469\n",
      "Epoch 11/100 | Train Loss: 33493.8569 | Val Loss: 98.4914 | R²: 0.6159 | RMSE: 21748.4824\n",
      "Epoch 12/100 | Train Loss: 32153.3194 | Val Loss: 99.8107 | R²: 0.6031 | RMSE: 22473.5801\n",
      "Epoch 13/100 | Train Loss: 29908.3898 | Val Loss: 98.8791 | R²: 0.6156 | RMSE: 21763.4922\n",
      "Epoch 14/100 | Train Loss: 30097.1299 | Val Loss: 119.9150 | R²: 0.4979 | RMSE: 28426.5488\n",
      "Epoch 15/100 | Train Loss: 30204.1322 | Val Loss: 131091.3439 | R²: -130949694.3682 | RMSE: 7414418505728.0000\n",
      "Epoch 16/100 | Train Loss: 27132.3204 | Val Loss: 103.8515 | R²: 0.5683 | RMSE: 24441.5254\n",
      "Epoch 17/100 | Train Loss: 27312.9584 | Val Loss: 96.0549 | R²: 0.6389 | RMSE: 20445.7070\n",
      "Epoch 18/100 | Train Loss: 27561.7610 | Val Loss: 94.4391 | R²: 0.6191 | RMSE: 21564.4238\n",
      "Epoch 19/100 | Train Loss: 28027.0080 | Val Loss: 92.5366 | R²: 0.6561 | RMSE: 19474.0879\n",
      "Epoch 20/100 | Train Loss: 27798.9377 | Val Loss: 97.3820 | R²: 0.6441 | RMSE: 20150.1367\n",
      "Epoch 21/100 | Train Loss: 27952.5635 | Val Loss: 96.6678 | R²: 0.6466 | RMSE: 20010.0039\n",
      "Epoch 22/100 | Train Loss: 26305.5622 | Val Loss: 93.8966 | R²: 0.6570 | RMSE: 19419.9551\n",
      "Epoch 23/100 | Train Loss: 29450.5696 | Val Loss: 91.0072 | R²: 0.6624 | RMSE: 19113.4590\n",
      "Epoch 24/100 | Train Loss: 27278.2675 | Val Loss: 111.1337 | R²: 0.5659 | RMSE: 24580.4902\n",
      "Epoch 25/100 | Train Loss: 27525.6599 | Val Loss: 100.0771 | R²: 0.6349 | RMSE: 20674.5879\n",
      "Epoch 26/100 | Train Loss: 27401.6370 | Val Loss: 108.6565 | R²: 0.2159 | RMSE: 44394.2812\n",
      "Epoch 27/100 | Train Loss: 27284.9747 | Val Loss: 95.6070 | R²: 0.6457 | RMSE: 20062.4531\n",
      "Epoch 28/100 | Train Loss: 26780.2366 | Val Loss: 91.5225 | R²: 0.6477 | RMSE: 19945.0625\n",
      "Epoch 29/100 | Train Loss: 27369.1888 | Val Loss: 94.0467 | R²: 0.6548 | RMSE: 19547.5176\n",
      "Epoch 30/100 | Train Loss: 29687.3615 | Val Loss: 92.4041 | R²: 0.6451 | RMSE: 20097.0625\n",
      "Epoch 31/100 | Train Loss: 26542.3339 | Val Loss: 96.2269 | R²: 0.6154 | RMSE: 21778.4863\n",
      "Epoch 32/100 | Train Loss: 25831.6281 | Val Loss: 106.3510 | R²: 0.5125 | RMSE: 27601.4375\n",
      "Epoch 33/100 | Train Loss: 28122.4561 | Val Loss: 91.2575 | R²: 0.6607 | RMSE: 19210.3066\n",
      "Early stopping at epoch 33\n",
      "\n",
      "Training model 3/5 with seed 62\n",
      "Epoch 1/100 | Train Loss: 89901.8542 | Val Loss: 145.8710 | R²: -0.0666 | RMSE: 59871.9180\n",
      "Epoch 2/100 | Train Loss: 86052.9585 | Val Loss: 142.6121 | R²: -0.0204 | RMSE: 57278.1445\n",
      "Epoch 3/100 | Train Loss: 75128.5825 | Val Loss: 136.9566 | R²: 0.0675 | RMSE: 52347.1250\n",
      "Epoch 4/100 | Train Loss: 74300.7804 | Val Loss: 132.2757 | R²: 0.1666 | RMSE: 46785.1250\n",
      "Epoch 5/100 | Train Loss: 68974.0868 | Val Loss: 126.0076 | R²: 0.2238 | RMSE: 43572.6758\n",
      "Epoch 6/100 | Train Loss: 61981.7040 | Val Loss: 120.9602 | R²: 0.2863 | RMSE: 40062.2188\n",
      "Epoch 7/100 | Train Loss: 53770.2910 | Val Loss: 232.7802 | R²: -19.3107 | RMSE: 1140132.0000\n",
      "Epoch 8/100 | Train Loss: 51476.6157 | Val Loss: 209.6060 | R²: -0.4358 | RMSE: 80598.1484\n",
      "Epoch 9/100 | Train Loss: 49791.9641 | Val Loss: 149.1731 | R²: 0.1512 | RMSE: 47649.0625\n",
      "Epoch 10/100 | Train Loss: 43842.2067 | Val Loss: 106.2923 | R²: 0.5207 | RMSE: 26904.8516\n",
      "Epoch 11/100 | Train Loss: 36430.1041 | Val Loss: 127.4099 | R²: -0.9057 | RMSE: 106975.4219\n",
      "Epoch 12/100 | Train Loss: 35145.6874 | Val Loss: 96.7961 | R²: 0.6137 | RMSE: 21685.8730\n",
      "Epoch 13/100 | Train Loss: 33008.6133 | Val Loss: 98.8903 | R²: 0.6235 | RMSE: 21135.9961\n",
      "Epoch 14/100 | Train Loss: 33188.9530 | Val Loss: 112.4500 | R²: 0.5815 | RMSE: 23495.0469\n",
      "Epoch 15/100 | Train Loss: 30914.8831 | Val Loss: 104.1130 | R²: 0.1803 | RMSE: 46013.4062\n",
      "Epoch 16/100 | Train Loss: 30462.5902 | Val Loss: 91.3756 | R²: 0.5598 | RMSE: 24709.2207\n",
      "Epoch 17/100 | Train Loss: 28337.2989 | Val Loss: 88.0024 | R²: 0.6518 | RMSE: 19546.1230\n",
      "Epoch 18/100 | Train Loss: 29464.5670 | Val Loss: 86.4136 | R²: 0.6990 | RMSE: 16894.7070\n",
      "Epoch 19/100 | Train Loss: 28862.3622 | Val Loss: 104.2980 | R²: 0.5515 | RMSE: 25173.8301\n",
      "Epoch 20/100 | Train Loss: 27926.1133 | Val Loss: 90.7694 | R²: 0.6780 | RMSE: 18073.4551\n",
      "Epoch 21/100 | Train Loss: 26533.1868 | Val Loss: 94.2688 | R²: 0.3298 | RMSE: 37619.8555\n",
      "Epoch 22/100 | Train Loss: 27554.6762 | Val Loss: 153.7956 | R²: -6.9277 | RMSE: 445019.3750\n",
      "Epoch 23/100 | Train Loss: 27656.1142 | Val Loss: 92.0765 | R²: 0.6329 | RMSE: 20607.5293\n",
      "Epoch 24/100 | Train Loss: 27013.4572 | Val Loss: 88.5385 | R²: 0.6920 | RMSE: 17287.2363\n",
      "Epoch 25/100 | Train Loss: 28294.9625 | Val Loss: 85.5554 | R²: 0.6867 | RMSE: 17589.2676\n",
      "Epoch 26/100 | Train Loss: 27194.0926 | Val Loss: 91.6018 | R²: 0.6865 | RMSE: 17596.6660\n",
      "Epoch 27/100 | Train Loss: 27009.2315 | Val Loss: 84.3039 | R²: 0.6890 | RMSE: 17457.5430\n",
      "Epoch 28/100 | Train Loss: 27278.3052 | Val Loss: 85.9047 | R²: 0.6809 | RMSE: 17915.3008\n",
      "Epoch 29/100 | Train Loss: 28514.5554 | Val Loss: 90.2399 | R²: 0.6268 | RMSE: 20950.5137\n",
      "Epoch 30/100 | Train Loss: 28291.5580 | Val Loss: 91.8809 | R²: 0.6824 | RMSE: 17827.8418\n",
      "Epoch 31/100 | Train Loss: 29127.3384 | Val Loss: 85.8026 | R²: 0.7034 | RMSE: 16652.0820\n",
      "Epoch 32/100 | Train Loss: 26832.8217 | Val Loss: 87.0118 | R²: 0.7010 | RMSE: 16781.4727\n",
      "Epoch 33/100 | Train Loss: 29730.9992 | Val Loss: 98.8168 | R²: 0.5812 | RMSE: 23511.7617\n",
      "Epoch 34/100 | Train Loss: 28544.0055 | Val Loss: 82.0474 | R²: 0.7005 | RMSE: 16814.6230\n",
      "Epoch 35/100 | Train Loss: 27479.6042 | Val Loss: 82.9175 | R²: 0.7110 | RMSE: 16221.2842\n",
      "Epoch 36/100 | Train Loss: 27840.7907 | Val Loss: 114.7183 | R²: 0.4749 | RMSE: 29474.0859\n",
      "Epoch 37/100 | Train Loss: 26444.7514 | Val Loss: 82.3615 | R²: 0.6973 | RMSE: 16992.8086\n",
      "Epoch 38/100 | Train Loss: 27548.7141 | Val Loss: 85.6037 | R²: 0.7062 | RMSE: 16492.0137\n",
      "Epoch 39/100 | Train Loss: 27845.8034 | Val Loss: 83.4610 | R²: 0.6912 | RMSE: 17334.7891\n",
      "Epoch 40/100 | Train Loss: 26580.1304 | Val Loss: 84.6525 | R²: 0.6991 | RMSE: 16888.7207\n",
      "Epoch 41/100 | Train Loss: 27434.7444 | Val Loss: 93.5185 | R²: 0.6077 | RMSE: 22021.4238\n",
      "Epoch 42/100 | Train Loss: 26499.6789 | Val Loss: 83.6658 | R²: 0.7104 | RMSE: 16258.4746\n",
      "Epoch 43/100 | Train Loss: 28738.2388 | Val Loss: 83.5292 | R²: 0.6934 | RMSE: 17212.0664\n",
      "Epoch 44/100 | Train Loss: 28011.5750 | Val Loss: 84.1663 | R²: 0.7066 | RMSE: 16472.3652\n",
      "Early stopping at epoch 44\n",
      "\n",
      "Training model 4/5 with seed 72\n",
      "Epoch 1/100 | Train Loss: 86527.8998 | Val Loss: 146.8847 | R²: -0.0774 | RMSE: 60570.5664\n",
      "Epoch 2/100 | Train Loss: 83264.8626 | Val Loss: 142.9816 | R²: -0.0193 | RMSE: 57307.0234\n",
      "Epoch 3/100 | Train Loss: 74366.0637 | Val Loss: 137.6629 | R²: 0.0607 | RMSE: 52809.2227\n",
      "Epoch 4/100 | Train Loss: 72172.1008 | Val Loss: 135.1316 | R²: 0.1279 | RMSE: 49028.7734\n",
      "Epoch 5/100 | Train Loss: 64751.3625 | Val Loss: 126.3189 | R²: 0.2300 | RMSE: 43288.4648\n",
      "Epoch 6/100 | Train Loss: 57364.5656 | Val Loss: 123.3164 | R²: 0.3520 | RMSE: 36430.3086\n",
      "Epoch 7/100 | Train Loss: 54787.9171 | Val Loss: 125.9790 | R²: 0.2686 | RMSE: 41121.7734\n",
      "Epoch 8/100 | Train Loss: 47518.2757 | Val Loss: 111.3714 | R²: 0.4651 | RMSE: 30072.3555\n",
      "Epoch 9/100 | Train Loss: 44185.0799 | Val Loss: 119.9476 | R²: 0.3816 | RMSE: 34767.2109\n",
      "Epoch 10/100 | Train Loss: 41241.2555 | Val Loss: 102.7222 | R²: 0.5504 | RMSE: 25274.9355\n",
      "Epoch 11/100 | Train Loss: 34835.2845 | Val Loss: 104.7159 | R²: 0.5740 | RMSE: 23948.5508\n",
      "Epoch 12/100 | Train Loss: 36192.8394 | Val Loss: 98.4211 | R²: 0.6052 | RMSE: 22193.6621\n",
      "Epoch 13/100 | Train Loss: 33288.6411 | Val Loss: 167.9365 | R²: 0.0720 | RMSE: 52171.2188\n",
      "Epoch 14/100 | Train Loss: 31665.7904 | Val Loss: 107.6810 | R²: 0.5969 | RMSE: 22663.8379\n",
      "Epoch 15/100 | Train Loss: 29506.8664 | Val Loss: 98.4453 | R²: 0.6485 | RMSE: 19762.9102\n",
      "Epoch 16/100 | Train Loss: 29880.9573 | Val Loss: 111.2790 | R²: 0.5839 | RMSE: 23395.4785\n",
      "Epoch 17/100 | Train Loss: 27506.0142 | Val Loss: 96.4719 | R²: 0.6139 | RMSE: 21706.4707\n",
      "Epoch 18/100 | Train Loss: 28032.6011 | Val Loss: 101.7170 | R²: 0.5543 | RMSE: 25054.6758\n",
      "Epoch 19/100 | Train Loss: 30091.0620 | Val Loss: 91.1846 | R²: 0.6728 | RMSE: 18393.0137\n",
      "Epoch 20/100 | Train Loss: 26291.7918 | Val Loss: 98.5624 | R²: 0.6371 | RMSE: 20400.5137\n",
      "Epoch 21/100 | Train Loss: 26742.6427 | Val Loss: 93.7602 | R²: 0.6040 | RMSE: 22263.2070\n",
      "Epoch 22/100 | Train Loss: 27310.4739 | Val Loss: 92.7856 | R²: 0.6270 | RMSE: 20972.3438\n",
      "Epoch 23/100 | Train Loss: 28786.9361 | Val Loss: 93.0640 | R²: 0.6775 | RMSE: 18133.3008\n",
      "Epoch 24/100 | Train Loss: 27412.7857 | Val Loss: 98.7060 | R²: 0.6518 | RMSE: 19575.0352\n",
      "Epoch 25/100 | Train Loss: 26650.2845 | Val Loss: 101.7487 | R²: 0.6141 | RMSE: 21697.0391\n",
      "Epoch 26/100 | Train Loss: 28749.9172 | Val Loss: 93.3588 | R²: 0.6705 | RMSE: 18522.0000\n",
      "Epoch 27/100 | Train Loss: 25850.7469 | Val Loss: 88.2696 | R²: 0.6890 | RMSE: 17486.9961\n",
      "Epoch 28/100 | Train Loss: 27411.2454 | Val Loss: 85.6087 | R²: 0.6797 | RMSE: 18006.3945\n",
      "Epoch 29/100 | Train Loss: 26638.5700 | Val Loss: 88.7552 | R²: 0.6562 | RMSE: 19329.2734\n",
      "Epoch 30/100 | Train Loss: 26973.7181 | Val Loss: 90.8960 | R²: 0.6750 | RMSE: 18274.0586\n",
      "Epoch 31/100 | Train Loss: 28192.2947 | Val Loss: 97.3858 | R²: 0.6499 | RMSE: 19679.8867\n",
      "Epoch 32/100 | Train Loss: 26939.4900 | Val Loss: 93.7714 | R²: 0.6721 | RMSE: 18435.1855\n",
      "Epoch 33/100 | Train Loss: 25528.1583 | Val Loss: 97.1825 | R²: 0.5937 | RMSE: 22841.6914\n",
      "Epoch 34/100 | Train Loss: 27502.6930 | Val Loss: 92.9529 | R²: 0.6675 | RMSE: 18691.2793\n",
      "Epoch 35/100 | Train Loss: 27862.0689 | Val Loss: 89.6282 | R²: 0.6741 | RMSE: 18323.6367\n",
      "Epoch 36/100 | Train Loss: 25952.2046 | Val Loss: 93.9762 | R²: 0.6671 | RMSE: 18716.8535\n",
      "Epoch 37/100 | Train Loss: 28380.0947 | Val Loss: 88.6600 | R²: 0.6749 | RMSE: 18276.9531\n",
      "Epoch 38/100 | Train Loss: 28969.8441 | Val Loss: 89.7019 | R²: 0.6866 | RMSE: 17617.8262\n",
      "Early stopping at epoch 38\n",
      "\n",
      "Training model 5/5 with seed 82\n",
      "Epoch 1/100 | Train Loss: 86935.9349 | Val Loss: 148.5679 | R²: -0.0922 | RMSE: 61729.0273\n",
      "Epoch 2/100 | Train Loss: 86983.7245 | Val Loss: 145.1179 | R²: -0.0414 | RMSE: 58856.4961\n",
      "Epoch 3/100 | Train Loss: 76617.7036 | Val Loss: 139.6679 | R²: 0.0378 | RMSE: 54377.9336\n",
      "Epoch 4/100 | Train Loss: 70635.0916 | Val Loss: 134.9606 | R²: 0.1023 | RMSE: 50736.2578\n",
      "Epoch 5/100 | Train Loss: 72697.5740 | Val Loss: 133.1485 | R²: -0.0085 | RMSE: 56998.8125\n",
      "Epoch 6/100 | Train Loss: 59556.0080 | Val Loss: 124.6743 | R²: 0.2876 | RMSE: 40260.2930\n",
      "Epoch 7/100 | Train Loss: 56108.1803 | Val Loss: 131.6475 | R²: -0.0315 | RMSE: 58295.9023\n",
      "Epoch 8/100 | Train Loss: 49394.5185 | Val Loss: 117.7361 | R²: 0.4019 | RMSE: 33801.9844\n",
      "Epoch 9/100 | Train Loss: 41710.5495 | Val Loss: 106.0663 | R²: 0.5488 | RMSE: 25500.2871\n",
      "Epoch 10/100 | Train Loss: 40792.3084 | Val Loss: 104.4548 | R²: 0.5191 | RMSE: 27175.7910\n",
      "Epoch 11/100 | Train Loss: 37069.1614 | Val Loss: 98.3839 | R²: 0.6134 | RMSE: 21849.5508\n",
      "Epoch 12/100 | Train Loss: 32913.2181 | Val Loss: 96.6650 | R²: 0.6193 | RMSE: 21514.6738\n",
      "Epoch 13/100 | Train Loss: 32654.1064 | Val Loss: 122.2220 | R²: 0.4526 | RMSE: 30937.9277\n",
      "Epoch 14/100 | Train Loss: 30221.0546 | Val Loss: 132.6581 | R²: -0.2215 | RMSE: 69033.6484\n",
      "Epoch 15/100 | Train Loss: 29887.6623 | Val Loss: 89.7274 | R²: 0.6695 | RMSE: 18680.7129\n",
      "Epoch 16/100 | Train Loss: 31075.8572 | Val Loss: 113.3006 | R²: 0.5904 | RMSE: 23149.1621\n",
      "Epoch 17/100 | Train Loss: 27035.6953 | Val Loss: 258.0514 | R²: -57.3921 | RMSE: 3300082.2500\n",
      "Epoch 18/100 | Train Loss: 27856.1359 | Val Loss: 91.6581 | R²: 0.6685 | RMSE: 18734.5938\n",
      "Epoch 19/100 | Train Loss: 26625.3511 | Val Loss: 409.4029 | R²: -119.9048 | RMSE: 6833043.5000\n",
      "Epoch 20/100 | Train Loss: 27262.8947 | Val Loss: 96.8029 | R²: 0.6623 | RMSE: 19084.9688\n",
      "Epoch 21/100 | Train Loss: 26692.5587 | Val Loss: 89.5908 | R²: 0.6828 | RMSE: 17928.4629\n",
      "Epoch 22/100 | Train Loss: 28034.5938 | Val Loss: 89.1037 | R²: 0.6900 | RMSE: 17521.7969\n",
      "Epoch 23/100 | Train Loss: 27372.2273 | Val Loss: 177.5064 | R²: -27.8613 | RMSE: 1631119.3750\n",
      "Epoch 24/100 | Train Loss: 26878.0269 | Val Loss: 93.9321 | R²: 0.6001 | RMSE: 22602.8438\n",
      "Epoch 25/100 | Train Loss: 27858.0556 | Val Loss: 108.2132 | R²: 0.3525 | RMSE: 36595.4883\n",
      "Epoch 26/100 | Train Loss: 26132.6897 | Val Loss: 89.0405 | R²: 0.6820 | RMSE: 17972.4922\n",
      "Epoch 27/100 | Train Loss: 27224.0502 | Val Loss: 92.7369 | R²: 0.6691 | RMSE: 18699.4746\n",
      "Epoch 28/100 | Train Loss: 29788.5744 | Val Loss: 96.5876 | R²: 0.6550 | RMSE: 19497.8008\n",
      "Epoch 29/100 | Train Loss: 27832.8616 | Val Loss: 131.2235 | R²: -2.4159 | RMSE: 193051.8281\n",
      "Epoch 30/100 | Train Loss: 26995.1792 | Val Loss: 91.0706 | R²: 0.6744 | RMSE: 18402.1309\n",
      "Epoch 31/100 | Train Loss: 27142.0042 | Val Loss: 89.1512 | R²: 0.6711 | RMSE: 18588.6270\n",
      "Epoch 32/100 | Train Loss: 26926.0363 | Val Loss: 114.8878 | R²: -2.0366 | RMSE: 171618.4375\n",
      "Epoch 33/100 | Train Loss: 28988.6912 | Val Loss: 92.0108 | R²: 0.6371 | RMSE: 20510.7969\n",
      "Epoch 34/100 | Train Loss: 24552.3525 | Val Loss: 92.5102 | R²: 0.6184 | RMSE: 21566.8027\n",
      "Epoch 35/100 | Train Loss: 28682.2640 | Val Loss: 89.0914 | R²: 0.6872 | RMSE: 17680.7168\n",
      "Epoch 36/100 | Train Loss: 26976.0549 | Val Loss: 97.4743 | R²: 0.5965 | RMSE: 22803.1484\n",
      "Early stopping at epoch 36\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Train ensemble models without Voltage\n",
    "##########################################\n",
    "\n",
    "models_v = train_ensemble(num_models=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Get predictions and targets\n",
    "################################################\n",
    "\n",
    "_, val_loader_v = get_data_loaders(seed=42, batch_size=16)\n",
    "all_preds_v, targets_v = get_ensemble_predictions(models_v, val_loader_v, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadeca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Individual Model Metrics********************************\n",
      "Model 1 → R2: 0.656, RMSE: 139.511, MAE: 93.568\n",
      "Model 2 → R2: 0.645, RMSE: 141.753, MAE: 92.991\n",
      "Model 3 → R2: 0.662, RMSE: 138.286, MAE: 91.725\n",
      "Model 4 → R2: 0.653, RMSE: 140.055, MAE: 94.373\n",
      "Model 5 → R2: 0.549, RMSE: 159.700, MAE: 106.748\n",
      "***********************************************************************\n",
      "\n",
      "Best Model : Model 3\n",
      "***********************************************************************\n",
      "Ensemble R2: 0.655\n",
      "Ensemble RMSE: 139.720\n",
      "Ensemble MAE: 91.421\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Evaluate the ensemble models for our CAPRI-CT without voltage\n",
    "#####################################################################\n",
    "\n",
    "resultv = evaluate_models(all_preds_v, targets_v)\n",
    "\n",
    "# View metrics\n",
    "print(f\"***************Individual Model Metrics********************************\")\n",
    "for m in resultv['individual_metrics']:\n",
    "    print(f\"Model {m['model_idx']+1} → R2: {m['r2']:.3f}, RMSE: {m['rmse']:.3f}, MAE: {m['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n",
    "\n",
    "print(f\"\\nBest Model : Model {resultv['best_model_index']+1}\")\n",
    "print(f\"***********************************************************************\")\n",
    "print(f\"Ensemble R2: {resultv['ensemble_metrics']['r2']:.3f}\")\n",
    "print(f\"Ensemble RMSE: {resultv['ensemble_metrics']['rmse']:.3f}\")\n",
    "print(f\"Ensemble MAE: {resultv['ensemble_metrics']['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/5 with seed 42\n",
      "Epoch 1/100 | Train Loss: 82066.4489 | Val Loss: 144.2555 | R²: -0.0414 | RMSE: 58874.7500\n",
      "Epoch 2/100 | Train Loss: 78887.7163 | Val Loss: 141.6847 | R²: -0.0122 | RMSE: 57227.5469\n",
      "Epoch 3/100 | Train Loss: 72884.5531 | Val Loss: 144.4774 | R²: -0.0373 | RMSE: 58642.7383\n",
      "Epoch 4/100 | Train Loss: 70664.4004 | Val Loss: 135.4380 | R²: 0.0825 | RMSE: 51871.7969\n",
      "Epoch 5/100 | Train Loss: 61308.8272 | Val Loss: 128.4142 | R²: 0.1884 | RMSE: 45885.8789\n",
      "Epoch 6/100 | Train Loss: 61332.8769 | Val Loss: 145.3295 | R²: 0.0552 | RMSE: 53412.3906\n",
      "Epoch 7/100 | Train Loss: 49873.8410 | Val Loss: 112.9105 | R²: 0.4131 | RMSE: 33182.0508\n",
      "Epoch 8/100 | Train Loss: 44315.7408 | Val Loss: 123.3406 | R²: 0.3355 | RMSE: 37568.9102\n",
      "Epoch 9/100 | Train Loss: 37972.0620 | Val Loss: 109.0942 | R²: 0.4753 | RMSE: 29665.8906\n",
      "Epoch 10/100 | Train Loss: 35385.0142 | Val Loss: 101.6780 | R²: 0.5902 | RMSE: 23170.0625\n",
      "Epoch 11/100 | Train Loss: 31936.4215 | Val Loss: 101.1761 | R²: 0.6069 | RMSE: 22222.4980\n",
      "Epoch 12/100 | Train Loss: 31216.9702 | Val Loss: 97.6160 | R²: 0.6335 | RMSE: 20720.0859\n",
      "Epoch 13/100 | Train Loss: 27579.3740 | Val Loss: 126.1709 | R²: 0.4020 | RMSE: 33806.6641\n",
      "Epoch 14/100 | Train Loss: 29238.7570 | Val Loss: 91.6063 | R²: 0.6745 | RMSE: 18403.0781\n",
      "Epoch 15/100 | Train Loss: 27205.9559 | Val Loss: 117.4988 | R²: 0.4797 | RMSE: 29415.1367\n",
      "Epoch 16/100 | Train Loss: 26797.6822 | Val Loss: 117.8011 | R²: 0.4311 | RMSE: 32164.1133\n",
      "Epoch 17/100 | Train Loss: 25797.0672 | Val Loss: 96.8091 | R²: 0.5937 | RMSE: 22968.5254\n",
      "Epoch 18/100 | Train Loss: 26772.2238 | Val Loss: 145.7427 | R²: 0.2571 | RMSE: 41998.8984\n",
      "Epoch 19/100 | Train Loss: 25938.8821 | Val Loss: 109.3303 | R²: 0.5919 | RMSE: 23071.6152\n",
      "Epoch 20/100 | Train Loss: 25160.3091 | Val Loss: 108.3781 | R²: 0.5432 | RMSE: 25825.5781\n",
      "Epoch 21/100 | Train Loss: 25025.1566 | Val Loss: 88.4334 | R²: 0.6871 | RMSE: 17689.4551\n",
      "Epoch 22/100 | Train Loss: 25883.6332 | Val Loss: 90.0928 | R²: 0.6896 | RMSE: 17547.5547\n",
      "Epoch 23/100 | Train Loss: 25526.8418 | Val Loss: 90.9791 | R²: 0.6691 | RMSE: 18707.6035\n",
      "Epoch 24/100 | Train Loss: 25855.2696 | Val Loss: 115.8368 | R²: 0.4550 | RMSE: 30811.0430\n",
      "Epoch 25/100 | Train Loss: 25356.2760 | Val Loss: 90.5230 | R²: 0.6835 | RMSE: 17894.1719\n",
      "Epoch 26/100 | Train Loss: 25019.0785 | Val Loss: 94.5012 | R²: 0.6586 | RMSE: 19300.0938\n",
      "Epoch 27/100 | Train Loss: 26102.6695 | Val Loss: 101.7515 | R²: 0.5697 | RMSE: 24327.3945\n",
      "Epoch 28/100 | Train Loss: 25845.2356 | Val Loss: 96.5277 | R²: 0.6131 | RMSE: 21873.0625\n",
      "Epoch 29/100 | Train Loss: 25832.8936 | Val Loss: 99.7083 | R²: 0.6009 | RMSE: 22563.5527\n",
      "Epoch 30/100 | Train Loss: 26745.8682 | Val Loss: 94.7936 | R²: 0.6322 | RMSE: 20794.5898\n",
      "Epoch 31/100 | Train Loss: 26884.0162 | Val Loss: 107.6757 | R²: 0.5333 | RMSE: 26387.2500\n",
      "Early stopping at epoch 31\n",
      "\n",
      "Training model 2/5 with seed 52\n",
      "Epoch 1/100 | Train Loss: 86971.4705 | Val Loss: 148.6172 | R²: -0.0958 | RMSE: 62047.3477\n",
      "Epoch 2/100 | Train Loss: 80628.4349 | Val Loss: 145.6491 | R²: -0.0403 | RMSE: 58903.5586\n",
      "Epoch 3/100 | Train Loss: 80540.7649 | Val Loss: 140.6634 | R²: 0.0473 | RMSE: 53942.4688\n",
      "Epoch 4/100 | Train Loss: 75586.9750 | Val Loss: 138.8069 | R²: 0.0898 | RMSE: 51538.2383\n",
      "Epoch 5/100 | Train Loss: 66994.4674 | Val Loss: 132.0392 | R²: 0.1876 | RMSE: 46000.9766\n",
      "Epoch 6/100 | Train Loss: 56943.7889 | Val Loss: 134.4642 | R²: 0.2016 | RMSE: 45206.1680\n",
      "Epoch 7/100 | Train Loss: 54530.0378 | Val Loss: 121.5569 | R²: 0.3493 | RMSE: 36844.4766\n",
      "Epoch 8/100 | Train Loss: 45891.6291 | Val Loss: 111.7519 | R²: 0.4651 | RMSE: 30285.9590\n",
      "Epoch 9/100 | Train Loss: 43424.1317 | Val Loss: 110.5376 | R²: 0.4819 | RMSE: 29333.1875\n",
      "Epoch 10/100 | Train Loss: 38000.6432 | Val Loss: 107.5931 | R²: 0.5277 | RMSE: 26739.0000\n",
      "Epoch 11/100 | Train Loss: 33602.2816 | Val Loss: 101.2166 | R²: 0.5973 | RMSE: 22802.1680\n",
      "Epoch 12/100 | Train Loss: 31221.9848 | Val Loss: 109.3516 | R²: 0.5503 | RMSE: 25461.6699\n",
      "Epoch 13/100 | Train Loss: 30920.0585 | Val Loss: 123.6219 | R²: 0.4705 | RMSE: 29981.0820\n",
      "Epoch 14/100 | Train Loss: 29377.1846 | Val Loss: 92.4068 | R²: 0.6598 | RMSE: 19262.6738\n",
      "Epoch 15/100 | Train Loss: 29155.6652 | Val Loss: 89.4555 | R²: 0.6778 | RMSE: 18245.1270\n",
      "Epoch 16/100 | Train Loss: 25665.5431 | Val Loss: 90.8355 | R²: 0.6764 | RMSE: 18320.9199\n",
      "Epoch 17/100 | Train Loss: 24754.5348 | Val Loss: 90.7816 | R²: 0.6709 | RMSE: 18631.4238\n",
      "Epoch 18/100 | Train Loss: 26721.2582 | Val Loss: 100.5323 | R²: 0.6268 | RMSE: 21127.9199\n",
      "Epoch 19/100 | Train Loss: 24193.5331 | Val Loss: 94.6970 | R²: 0.6085 | RMSE: 22167.0508\n",
      "Epoch 20/100 | Train Loss: 25323.2151 | Val Loss: 90.2425 | R²: 0.6778 | RMSE: 18241.7598\n",
      "Epoch 21/100 | Train Loss: 25978.5135 | Val Loss: 89.4866 | R²: 0.6799 | RMSE: 18124.8145\n",
      "Epoch 22/100 | Train Loss: 26212.2044 | Val Loss: 94.4704 | R²: 0.6579 | RMSE: 19368.8164\n",
      "Epoch 23/100 | Train Loss: 26507.4557 | Val Loss: 89.8642 | R²: 0.6827 | RMSE: 17962.9316\n",
      "Epoch 24/100 | Train Loss: 25288.1037 | Val Loss: 89.3169 | R²: 0.6893 | RMSE: 17593.3359\n",
      "Epoch 25/100 | Train Loss: 23982.4887 | Val Loss: 90.7848 | R²: 0.6798 | RMSE: 18131.3379\n",
      "Epoch 26/100 | Train Loss: 25502.6467 | Val Loss: 87.0121 | R²: 0.6865 | RMSE: 17750.7168\n",
      "Epoch 27/100 | Train Loss: 26161.1541 | Val Loss: 88.1533 | R²: 0.6901 | RMSE: 17546.9570\n",
      "Epoch 28/100 | Train Loss: 26078.7130 | Val Loss: 86.6363 | R²: 0.6871 | RMSE: 17715.8906\n",
      "Epoch 29/100 | Train Loss: 27032.7124 | Val Loss: 97.1828 | R²: 0.6427 | RMSE: 20229.2344\n",
      "Epoch 30/100 | Train Loss: 26402.3143 | Val Loss: 93.1052 | R²: 0.6463 | RMSE: 20024.4902\n",
      "Epoch 31/100 | Train Loss: 26263.0912 | Val Loss: 91.1739 | R²: 0.6701 | RMSE: 18677.3438\n",
      "Epoch 32/100 | Train Loss: 27007.1754 | Val Loss: 88.0055 | R²: 0.6874 | RMSE: 17699.0586\n",
      "Epoch 33/100 | Train Loss: 26163.4686 | Val Loss: 86.1861 | R²: 0.6919 | RMSE: 17442.6348\n",
      "Epoch 34/100 | Train Loss: 23791.7157 | Val Loss: 93.8777 | R²: 0.6226 | RMSE: 21370.0137\n",
      "Epoch 35/100 | Train Loss: 24779.9927 | Val Loss: 91.0792 | R²: 0.6790 | RMSE: 18172.4160\n",
      "Epoch 36/100 | Train Loss: 25952.1843 | Val Loss: 86.3225 | R²: 0.6886 | RMSE: 17631.8945\n",
      "Epoch 37/100 | Train Loss: 23784.7911 | Val Loss: 88.0224 | R²: 0.6896 | RMSE: 17577.4219\n",
      "Epoch 38/100 | Train Loss: 25245.0972 | Val Loss: 87.0997 | R²: 0.6860 | RMSE: 17779.1660\n",
      "Epoch 39/100 | Train Loss: 23313.5761 | Val Loss: 86.3489 | R²: 0.6924 | RMSE: 17416.6816\n",
      "Epoch 40/100 | Train Loss: 23490.9047 | Val Loss: 85.5221 | R²: 0.6954 | RMSE: 17245.8145\n",
      "Epoch 41/100 | Train Loss: 23987.3886 | Val Loss: 85.8832 | R²: 0.6946 | RMSE: 17293.3125\n",
      "Epoch 42/100 | Train Loss: 25431.2084 | Val Loss: 87.0678 | R²: 0.6922 | RMSE: 17425.1152\n",
      "Epoch 43/100 | Train Loss: 24696.5595 | Val Loss: 88.6345 | R²: 0.6894 | RMSE: 17588.9453\n",
      "Epoch 44/100 | Train Loss: 25964.5747 | Val Loss: 86.6761 | R²: 0.6910 | RMSE: 17493.7031\n",
      "Epoch 45/100 | Train Loss: 25767.0747 | Val Loss: 86.0537 | R²: 0.6964 | RMSE: 17188.8867\n",
      "Epoch 46/100 | Train Loss: 25126.6710 | Val Loss: 87.4573 | R²: 0.6815 | RMSE: 18036.0117\n",
      "Epoch 47/100 | Train Loss: 25720.1186 | Val Loss: 86.9166 | R²: 0.6766 | RMSE: 18312.8340\n",
      "Epoch 48/100 | Train Loss: 25389.7418 | Val Loss: 87.5859 | R²: 0.6889 | RMSE: 17614.7637\n",
      "Epoch 49/100 | Train Loss: 25300.9240 | Val Loss: 86.2902 | R²: 0.6891 | RMSE: 17601.0645\n",
      "Epoch 50/100 | Train Loss: 24394.0920 | Val Loss: 86.1048 | R²: 0.6890 | RMSE: 17610.5527\n",
      "Early stopping at epoch 50\n",
      "\n",
      "Training model 3/5 with seed 62\n",
      "Epoch 1/100 | Train Loss: 87395.5550 | Val Loss: 147.0345 | R²: -0.0829 | RMSE: 60788.6445\n",
      "Epoch 2/100 | Train Loss: 85372.1485 | Val Loss: 142.9849 | R²: -0.0191 | RMSE: 57208.9141\n",
      "Epoch 3/100 | Train Loss: 78467.7631 | Val Loss: 136.2115 | R²: 0.0823 | RMSE: 51516.9648\n",
      "Epoch 4/100 | Train Loss: 70126.7711 | Val Loss: 131.6808 | R²: 0.1492 | RMSE: 47757.8047\n",
      "Epoch 5/100 | Train Loss: 62514.8946 | Val Loss: 123.2984 | R²: 0.2635 | RMSE: 41344.4414\n",
      "Epoch 6/100 | Train Loss: 59762.0622 | Val Loss: 133.8162 | R²: 0.0917 | RMSE: 50988.3633\n",
      "Epoch 7/100 | Train Loss: 59313.5360 | Val Loss: 138.3151 | R²: 0.1865 | RMSE: 45664.4883\n",
      "Epoch 8/100 | Train Loss: 48958.9328 | Val Loss: 107.1583 | R²: 0.4687 | RMSE: 29823.7832\n",
      "Epoch 9/100 | Train Loss: 48995.3515 | Val Loss: 108.9090 | R²: 0.4607 | RMSE: 30272.6367\n",
      "Epoch 10/100 | Train Loss: 43444.0183 | Val Loss: 128.7716 | R²: 0.3449 | RMSE: 36776.3125\n",
      "Epoch 11/100 | Train Loss: 35355.2459 | Val Loss: 118.3044 | R²: 0.5284 | RMSE: 26472.3145\n",
      "Epoch 12/100 | Train Loss: 33000.4832 | Val Loss: 108.1555 | R²: 0.2864 | RMSE: 40055.3281\n",
      "Epoch 13/100 | Train Loss: 31241.3773 | Val Loss: 101.4648 | R²: 0.5718 | RMSE: 24034.4668\n",
      "Epoch 14/100 | Train Loss: 29941.6778 | Val Loss: 95.5501 | R²: 0.6325 | RMSE: 20631.7285\n",
      "Epoch 15/100 | Train Loss: 29213.5037 | Val Loss: 95.8998 | R²: 0.6179 | RMSE: 21451.3438\n",
      "Epoch 16/100 | Train Loss: 27550.5577 | Val Loss: 96.6212 | R²: 0.6377 | RMSE: 20337.3945\n",
      "Epoch 17/100 | Train Loss: 25226.8239 | Val Loss: 142.4969 | R²: -0.3971 | RMSE: 78424.0547\n",
      "Epoch 18/100 | Train Loss: 27117.3827 | Val Loss: 101.8373 | R²: 0.5815 | RMSE: 23492.5762\n",
      "Epoch 19/100 | Train Loss: 25374.8594 | Val Loss: 87.7850 | R²: 0.6734 | RMSE: 18332.3398\n",
      "Epoch 20/100 | Train Loss: 24409.4401 | Val Loss: 96.6042 | R²: 0.6509 | RMSE: 19598.8027\n",
      "Epoch 21/100 | Train Loss: 25469.2782 | Val Loss: 95.8465 | R²: 0.6027 | RMSE: 22301.2871\n",
      "Epoch 22/100 | Train Loss: 25871.9745 | Val Loss: 112.1223 | R²: 0.4995 | RMSE: 28098.0078\n",
      "Epoch 23/100 | Train Loss: 25597.1125 | Val Loss: 90.7147 | R²: 0.6766 | RMSE: 18155.4746\n",
      "Epoch 24/100 | Train Loss: 24882.7506 | Val Loss: 87.4283 | R²: 0.6825 | RMSE: 17823.0000\n",
      "Epoch 25/100 | Train Loss: 23946.0765 | Val Loss: 87.9692 | R²: 0.6700 | RMSE: 18526.5156\n",
      "Epoch 26/100 | Train Loss: 24133.1064 | Val Loss: 89.8892 | R²: 0.6619 | RMSE: 18976.6660\n",
      "Epoch 27/100 | Train Loss: 25059.9026 | Val Loss: 87.6996 | R²: 0.6735 | RMSE: 18329.9434\n",
      "Epoch 28/100 | Train Loss: 24360.0183 | Val Loss: 111.5651 | R²: 0.4705 | RMSE: 29723.3125\n",
      "Epoch 29/100 | Train Loss: 25632.3803 | Val Loss: 89.4426 | R²: 0.6628 | RMSE: 18928.1836\n",
      "Epoch 30/100 | Train Loss: 24612.3185 | Val Loss: 111.3834 | R²: 0.4550 | RMSE: 30593.4785\n",
      "Epoch 31/100 | Train Loss: 25531.3628 | Val Loss: 86.1699 | R²: 0.6785 | RMSE: 18048.9512\n",
      "Epoch 32/100 | Train Loss: 24370.6190 | Val Loss: 88.2055 | R²: 0.6721 | RMSE: 18406.0430\n",
      "Epoch 33/100 | Train Loss: 24649.8926 | Val Loss: 86.9673 | R²: 0.6729 | RMSE: 18362.3906\n",
      "Epoch 34/100 | Train Loss: 22480.2733 | Val Loss: 87.1398 | R²: 0.6777 | RMSE: 18093.3730\n",
      "Epoch 35/100 | Train Loss: 24336.4602 | Val Loss: 86.1330 | R²: 0.6803 | RMSE: 17948.6172\n",
      "Epoch 36/100 | Train Loss: 24173.7958 | Val Loss: 107.4976 | R²: 0.4948 | RMSE: 28357.2891\n",
      "Epoch 37/100 | Train Loss: 24122.9281 | Val Loss: 88.0020 | R²: 0.6722 | RMSE: 18399.0039\n",
      "Epoch 38/100 | Train Loss: 21437.6149 | Val Loss: 86.5759 | R²: 0.6815 | RMSE: 17880.0059\n",
      "Epoch 39/100 | Train Loss: 22700.5885 | Val Loss: 88.1620 | R²: 0.6751 | RMSE: 18239.9570\n",
      "Epoch 40/100 | Train Loss: 22303.2072 | Val Loss: 111.3427 | R²: 0.5296 | RMSE: 26402.9160\n",
      "Epoch 41/100 | Train Loss: 24884.0316 | Val Loss: 96.5534 | R²: 0.6342 | RMSE: 20535.8789\n",
      "Epoch 42/100 | Train Loss: 22720.1688 | Val Loss: 89.5305 | R²: 0.6678 | RMSE: 18649.6523\n",
      "Epoch 43/100 | Train Loss: 23570.1804 | Val Loss: 89.8516 | R²: 0.6708 | RMSE: 18478.8789\n",
      "Epoch 44/100 | Train Loss: 24334.9328 | Val Loss: 88.0806 | R²: 0.6743 | RMSE: 18281.6406\n",
      "Epoch 45/100 | Train Loss: 23563.0050 | Val Loss: 87.9304 | R²: 0.6720 | RMSE: 18410.5078\n",
      "Early stopping at epoch 45\n",
      "\n",
      "Training model 4/5 with seed 72\n",
      "Epoch 1/100 | Train Loss: 86343.1170 | Val Loss: 147.7536 | R²: -0.0877 | RMSE: 61149.1445\n",
      "Epoch 2/100 | Train Loss: 82260.1033 | Val Loss: 140.7772 | R²: 0.0146 | RMSE: 55399.0703\n",
      "Epoch 3/100 | Train Loss: 74976.4263 | Val Loss: 140.1888 | R²: 0.0341 | RMSE: 54303.5430\n",
      "Epoch 4/100 | Train Loss: 67781.9387 | Val Loss: 133.3193 | R²: 0.1185 | RMSE: 49558.2266\n",
      "Epoch 5/100 | Train Loss: 59835.8570 | Val Loss: 130.4671 | R²: 0.1860 | RMSE: 45765.2734\n",
      "Epoch 6/100 | Train Loss: 54987.7580 | Val Loss: 130.8494 | R²: 0.1618 | RMSE: 47122.0703\n",
      "Epoch 7/100 | Train Loss: 52549.8499 | Val Loss: 115.9107 | R²: 0.3932 | RMSE: 34116.3828\n",
      "Epoch 8/100 | Train Loss: 46743.6054 | Val Loss: 116.2214 | R²: 0.3873 | RMSE: 34448.3633\n",
      "Epoch 9/100 | Train Loss: 42752.1986 | Val Loss: 151.2977 | R²: -1.4687 | RMSE: 138789.7031\n",
      "Epoch 10/100 | Train Loss: 36275.3857 | Val Loss: 105.6446 | R²: 0.5321 | RMSE: 26306.7383\n",
      "Epoch 11/100 | Train Loss: 30749.9901 | Val Loss: 98.6370 | R²: 0.5900 | RMSE: 23050.9316\n",
      "Epoch 12/100 | Train Loss: 32311.0551 | Val Loss: 100.9894 | R²: 0.5549 | RMSE: 25024.4863\n",
      "Epoch 13/100 | Train Loss: 29809.5751 | Val Loss: 94.4851 | R²: 0.6359 | RMSE: 20467.4121\n",
      "Epoch 14/100 | Train Loss: 28450.5059 | Val Loss: 117.3190 | R²: 0.4509 | RMSE: 30871.6367\n",
      "Epoch 15/100 | Train Loss: 28914.8558 | Val Loss: 99.0009 | R²: 0.5671 | RMSE: 24334.7969\n",
      "Epoch 16/100 | Train Loss: 26680.8074 | Val Loss: 93.3773 | R²: 0.6364 | RMSE: 20444.1660\n",
      "Epoch 17/100 | Train Loss: 25152.2329 | Val Loss: 94.4288 | R²: 0.6169 | RMSE: 21537.5801\n",
      "Epoch 18/100 | Train Loss: 26560.7377 | Val Loss: 92.4253 | R²: 0.6350 | RMSE: 20522.0293\n",
      "Epoch 19/100 | Train Loss: 25825.3020 | Val Loss: 101.0250 | R²: 0.3959 | RMSE: 33963.3594\n",
      "Epoch 20/100 | Train Loss: 25560.4500 | Val Loss: 90.1435 | R²: 0.6474 | RMSE: 19820.6895\n",
      "Epoch 21/100 | Train Loss: 23452.9208 | Val Loss: 108.8394 | R²: 0.5041 | RMSE: 27879.4824\n",
      "Epoch 22/100 | Train Loss: 26114.6240 | Val Loss: 109.3255 | R²: 0.4989 | RMSE: 28172.8066\n",
      "Epoch 23/100 | Train Loss: 26941.1374 | Val Loss: 90.3447 | R²: 0.6569 | RMSE: 19286.4395\n",
      "Epoch 24/100 | Train Loss: 24679.1549 | Val Loss: 93.0965 | R²: 0.6332 | RMSE: 20623.5176\n",
      "Epoch 25/100 | Train Loss: 23936.2560 | Val Loss: 87.6182 | R²: 0.6602 | RMSE: 19105.0801\n",
      "Epoch 26/100 | Train Loss: 25593.9384 | Val Loss: 91.1274 | R²: 0.6322 | RMSE: 20675.6426\n",
      "Epoch 27/100 | Train Loss: 24653.4707 | Val Loss: 91.0885 | R²: 0.6551 | RMSE: 19388.1738\n",
      "Epoch 28/100 | Train Loss: 27508.8786 | Val Loss: 103.4132 | R²: 0.5774 | RMSE: 23761.1504\n",
      "Epoch 29/100 | Train Loss: 24137.3880 | Val Loss: 108.2737 | R²: -0.4692 | RMSE: 82595.4531\n",
      "Epoch 30/100 | Train Loss: 24629.5695 | Val Loss: 91.0673 | R²: 0.6508 | RMSE: 19633.7109\n",
      "Epoch 31/100 | Train Loss: 24797.7041 | Val Loss: 88.6688 | R²: 0.6577 | RMSE: 19246.1621\n",
      "Epoch 32/100 | Train Loss: 23916.9898 | Val Loss: 87.1356 | R²: 0.6619 | RMSE: 19005.1270\n",
      "Epoch 33/100 | Train Loss: 23102.2509 | Val Loss: 89.4121 | R²: 0.6545 | RMSE: 19423.7930\n",
      "Epoch 34/100 | Train Loss: 23398.1849 | Val Loss: 89.7454 | R²: 0.6587 | RMSE: 19190.1035\n",
      "Epoch 35/100 | Train Loss: 24507.3341 | Val Loss: 89.4097 | R²: 0.6565 | RMSE: 19309.5840\n",
      "Epoch 36/100 | Train Loss: 22414.1626 | Val Loss: 88.4018 | R²: 0.6556 | RMSE: 19364.5957\n",
      "Epoch 37/100 | Train Loss: 22769.5977 | Val Loss: 89.3583 | R²: 0.6522 | RMSE: 19550.9023\n",
      "Epoch 38/100 | Train Loss: 23828.8378 | Val Loss: 88.2027 | R²: 0.6533 | RMSE: 19489.8848\n",
      "Epoch 39/100 | Train Loss: 23546.8125 | Val Loss: 88.4239 | R²: 0.6572 | RMSE: 19270.4414\n",
      "Epoch 40/100 | Train Loss: 20705.3520 | Val Loss: 86.6590 | R²: 0.6646 | RMSE: 18856.5586\n",
      "Epoch 41/100 | Train Loss: 23375.8170 | Val Loss: 88.1722 | R²: 0.6590 | RMSE: 19170.4297\n",
      "Epoch 42/100 | Train Loss: 23060.9214 | Val Loss: 87.8976 | R²: 0.6590 | RMSE: 19173.5430\n",
      "Epoch 43/100 | Train Loss: 24693.5710 | Val Loss: 86.6678 | R²: 0.6618 | RMSE: 19011.4668\n",
      "Epoch 44/100 | Train Loss: 22947.1517 | Val Loss: 95.2078 | R²: 0.6084 | RMSE: 22015.5859\n",
      "Epoch 45/100 | Train Loss: 23523.6623 | Val Loss: 90.2160 | R²: 0.6549 | RMSE: 19404.1094\n",
      "Epoch 46/100 | Train Loss: 22253.0556 | Val Loss: 87.6803 | R²: 0.6578 | RMSE: 19237.0879\n",
      "Epoch 47/100 | Train Loss: 22711.9204 | Val Loss: 90.0206 | R²: 0.6505 | RMSE: 19645.9570\n",
      "Epoch 48/100 | Train Loss: 23905.0662 | Val Loss: 86.6963 | R²: 0.6631 | RMSE: 18939.6914\n",
      "Epoch 49/100 | Train Loss: 22575.0747 | Val Loss: 88.4606 | R²: 0.6607 | RMSE: 19075.2832\n",
      "Epoch 50/100 | Train Loss: 23383.6244 | Val Loss: 86.6223 | R²: 0.6624 | RMSE: 18977.0371\n",
      "Epoch 51/100 | Train Loss: 23389.4237 | Val Loss: 89.8995 | R²: 0.6561 | RMSE: 19335.0215\n",
      "Epoch 52/100 | Train Loss: 23406.3977 | Val Loss: 91.6010 | R²: 0.6516 | RMSE: 19584.1191\n",
      "Epoch 53/100 | Train Loss: 22448.0079 | Val Loss: 86.5780 | R²: 0.6658 | RMSE: 18789.0293\n",
      "Epoch 54/100 | Train Loss: 25167.2047 | Val Loss: 86.4719 | R²: 0.6598 | RMSE: 19124.2461\n",
      "Epoch 55/100 | Train Loss: 24376.2486 | Val Loss: 86.7071 | R²: 0.6636 | RMSE: 18909.9355\n",
      "Epoch 56/100 | Train Loss: 23402.0476 | Val Loss: 91.2742 | R²: 0.6495 | RMSE: 19704.0898\n",
      "Epoch 57/100 | Train Loss: 22484.9754 | Val Loss: 87.9039 | R²: 0.6584 | RMSE: 19206.3457\n",
      "Epoch 58/100 | Train Loss: 21566.9125 | Val Loss: 86.9890 | R²: 0.6597 | RMSE: 19130.4336\n",
      "Epoch 59/100 | Train Loss: 21562.2352 | Val Loss: 86.2181 | R²: 0.6621 | RMSE: 18995.1855\n",
      "Epoch 60/100 | Train Loss: 23147.2997 | Val Loss: 86.2414 | R²: 0.6662 | RMSE: 18767.3633\n",
      "Epoch 61/100 | Train Loss: 25103.7508 | Val Loss: 87.1242 | R²: 0.6650 | RMSE: 18831.2754\n",
      "Epoch 62/100 | Train Loss: 20223.0542 | Val Loss: 86.8484 | R²: 0.6605 | RMSE: 19084.0547\n",
      "Epoch 63/100 | Train Loss: 21848.8105 | Val Loss: 90.0261 | R²: 0.6435 | RMSE: 20045.0234\n",
      "Epoch 64/100 | Train Loss: 23315.3885 | Val Loss: 86.0889 | R²: 0.6639 | RMSE: 18892.8633\n",
      "Epoch 65/100 | Train Loss: 22296.5197 | Val Loss: 88.4365 | R²: 0.6510 | RMSE: 19622.9629\n",
      "Epoch 66/100 | Train Loss: 24005.0529 | Val Loss: 86.2030 | R²: 0.6583 | RMSE: 19210.7168\n",
      "Epoch 67/100 | Train Loss: 22941.9089 | Val Loss: 92.5297 | R²: 0.6389 | RMSE: 20302.5879\n",
      "Epoch 68/100 | Train Loss: 22035.5827 | Val Loss: 88.5111 | R²: 0.6563 | RMSE: 19324.6719\n",
      "Epoch 69/100 | Train Loss: 22263.7301 | Val Loss: 87.8908 | R²: 0.6633 | RMSE: 18931.8125\n",
      "Epoch 70/100 | Train Loss: 22507.1441 | Val Loss: 87.0360 | R²: 0.6629 | RMSE: 18949.5234\n",
      "Epoch 71/100 | Train Loss: 20758.7033 | Val Loss: 86.4338 | R²: 0.6675 | RMSE: 18692.2168\n",
      "Epoch 72/100 | Train Loss: 24034.1584 | Val Loss: 88.2245 | R²: 0.6612 | RMSE: 19046.0254\n",
      "Epoch 73/100 | Train Loss: 23193.0605 | Val Loss: 91.6349 | R²: 0.6501 | RMSE: 19672.3633\n",
      "Epoch 74/100 | Train Loss: 22912.7071 | Val Loss: 88.2708 | R²: 0.6641 | RMSE: 18884.5723\n",
      "Early stopping at epoch 74\n",
      "\n",
      "Training model 5/5 with seed 82\n",
      "Epoch 1/100 | Train Loss: 87039.4752 | Val Loss: 147.2586 | R²: -0.0742 | RMSE: 60708.2422\n",
      "Epoch 2/100 | Train Loss: 85498.7485 | Val Loss: 143.9190 | R²: -0.0265 | RMSE: 58015.2812\n",
      "Epoch 3/100 | Train Loss: 77711.8977 | Val Loss: 140.4533 | R²: 0.0239 | RMSE: 55166.6875\n",
      "Epoch 4/100 | Train Loss: 73746.3376 | Val Loss: 133.5767 | R²: 0.1241 | RMSE: 49499.9609\n",
      "Epoch 5/100 | Train Loss: 69686.8112 | Val Loss: 134.1998 | R²: 0.1229 | RMSE: 49568.2305\n",
      "Epoch 6/100 | Train Loss: 59281.5098 | Val Loss: 127.8584 | R²: 0.2198 | RMSE: 44091.1445\n",
      "Epoch 7/100 | Train Loss: 57825.5680 | Val Loss: 185.5178 | R²: -0.2379 | RMSE: 69963.8516\n",
      "Epoch 8/100 | Train Loss: 50292.3043 | Val Loss: 116.8307 | R²: 0.3651 | RMSE: 35879.3750\n",
      "Epoch 9/100 | Train Loss: 45116.5201 | Val Loss: 113.6904 | R²: 0.4126 | RMSE: 33198.6445\n",
      "Epoch 10/100 | Train Loss: 39369.6064 | Val Loss: 107.0954 | R²: 0.4930 | RMSE: 28655.6172\n",
      "Epoch 11/100 | Train Loss: 36778.7382 | Val Loss: 120.9077 | R²: 0.4135 | RMSE: 33147.1875\n",
      "Epoch 12/100 | Train Loss: 33594.2176 | Val Loss: 114.5749 | R²: 0.4636 | RMSE: 30316.4668\n",
      "Epoch 13/100 | Train Loss: 30213.8723 | Val Loss: 108.2650 | R²: 0.5451 | RMSE: 25708.5879\n",
      "Epoch 14/100 | Train Loss: 27631.5326 | Val Loss: 92.9768 | R²: 0.6562 | RMSE: 19431.9609\n",
      "Epoch 15/100 | Train Loss: 28010.4925 | Val Loss: 102.0446 | R²: 0.5613 | RMSE: 24794.7598\n",
      "Epoch 16/100 | Train Loss: 27983.3178 | Val Loss: 106.4652 | R²: 0.5499 | RMSE: 25435.3379\n",
      "Epoch 17/100 | Train Loss: 26821.1988 | Val Loss: 108.0522 | R²: 0.5177 | RMSE: 27256.2168\n",
      "Epoch 18/100 | Train Loss: 25793.3158 | Val Loss: 92.3682 | R²: 0.6316 | RMSE: 20821.8320\n",
      "Epoch 19/100 | Train Loss: 24298.7743 | Val Loss: 85.8198 | R²: 0.6742 | RMSE: 18412.0527\n",
      "Epoch 20/100 | Train Loss: 24675.2916 | Val Loss: 94.3538 | R²: 0.5972 | RMSE: 22762.7891\n",
      "Epoch 21/100 | Train Loss: 26039.0057 | Val Loss: 86.8415 | R²: 0.6812 | RMSE: 18016.6094\n",
      "Epoch 22/100 | Train Loss: 24834.8890 | Val Loss: 86.5253 | R²: 0.6812 | RMSE: 18018.9355\n",
      "Epoch 23/100 | Train Loss: 25814.3537 | Val Loss: 87.0181 | R²: 0.6772 | RMSE: 18242.1133\n",
      "Epoch 24/100 | Train Loss: 23427.1848 | Val Loss: 92.3720 | R²: 0.6357 | RMSE: 20591.0840\n",
      "Epoch 25/100 | Train Loss: 25214.7727 | Val Loss: 99.4144 | R²: 0.6157 | RMSE: 21718.7793\n",
      "Epoch 26/100 | Train Loss: 25501.2862 | Val Loss: 110.0653 | R²: 0.5176 | RMSE: 27263.7539\n",
      "Epoch 27/100 | Train Loss: 25354.4154 | Val Loss: 88.1918 | R²: 0.6778 | RMSE: 18208.7617\n",
      "Epoch 28/100 | Train Loss: 26999.9094 | Val Loss: 85.8065 | R²: 0.6811 | RMSE: 18021.9199\n",
      "Epoch 29/100 | Train Loss: 25957.2898 | Val Loss: 87.2710 | R²: 0.6601 | RMSE: 19208.4199\n",
      "Epoch 30/100 | Train Loss: 24756.7426 | Val Loss: 93.2855 | R²: 0.6237 | RMSE: 21269.4531\n",
      "Epoch 31/100 | Train Loss: 25030.6071 | Val Loss: 93.0470 | R²: 0.6619 | RMSE: 19107.7109\n",
      "Epoch 32/100 | Train Loss: 24661.7013 | Val Loss: 85.0706 | R²: 0.6833 | RMSE: 17898.3438\n",
      "Epoch 33/100 | Train Loss: 25876.2503 | Val Loss: 117.9618 | R²: 0.3984 | RMSE: 34002.6523\n",
      "Epoch 34/100 | Train Loss: 23882.9209 | Val Loss: 84.9707 | R²: 0.6902 | RMSE: 17507.3848\n",
      "Epoch 35/100 | Train Loss: 25142.1493 | Val Loss: 85.8601 | R²: 0.6806 | RMSE: 18051.2285\n",
      "Epoch 36/100 | Train Loss: 24563.2351 | Val Loss: 91.8784 | R²: 0.6350 | RMSE: 20629.9160\n",
      "Epoch 37/100 | Train Loss: 23428.4151 | Val Loss: 90.5531 | R²: 0.6675 | RMSE: 18791.2637\n",
      "Epoch 38/100 | Train Loss: 25207.9131 | Val Loss: 106.0809 | R²: 0.5437 | RMSE: 25789.9922\n",
      "Epoch 39/100 | Train Loss: 25230.7505 | Val Loss: 86.8836 | R²: 0.6717 | RMSE: 18552.3066\n",
      "Epoch 40/100 | Train Loss: 23189.2981 | Val Loss: 86.2750 | R²: 0.6714 | RMSE: 18570.1230\n",
      "Epoch 41/100 | Train Loss: 23319.9569 | Val Loss: 85.6479 | R²: 0.6724 | RMSE: 18517.4062\n",
      "Epoch 42/100 | Train Loss: 26451.9911 | Val Loss: 88.3911 | R²: 0.6538 | RMSE: 19566.9355\n",
      "Epoch 43/100 | Train Loss: 26094.3471 | Val Loss: 86.9979 | R²: 0.6809 | RMSE: 18033.8359\n",
      "Epoch 44/100 | Train Loss: 22221.3668 | Val Loss: 86.0008 | R²: 0.6796 | RMSE: 18106.2891\n",
      "Early stopping at epoch 44\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Train ensemble models without time\n",
    "##########################################\n",
    "\n",
    "models_t = train_ensemble(num_models=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da790703",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Get predictions and targets\n",
    "################################################\n",
    "_, val_loader_t = get_data_loaders(seed=42, batch_size=16)\n",
    "all_preds_t, targets_t = get_ensemble_predictions(models_t, val_loader_t, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Individual Model Metrics********************************\n",
      "Model 1 → R2: 0.536, RMSE: 162.020, MAE: 106.878\n",
      "Model 2 → R2: 0.686, RMSE: 133.207, MAE: 88.050\n",
      "Model 3 → R2: 0.685, RMSE: 133.491, MAE: 87.294\n",
      "Model 4 → R2: 0.692, RMSE: 132.015, MAE: 87.603\n",
      "Model 5 → R2: 0.697, RMSE: 130.843, MAE: 86.152\n",
      "***********************************************************************\n",
      "\n",
      "Best Model : Model 5\n",
      "***********************************************************************\n",
      "Ensemble R2: 0.684\n",
      "Ensemble RMSE: 133.561\n",
      "Ensemble MAE: 87.209\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Evaluate the ensemble models for our CAPRI-CT without time\n",
    "#####################################################################\n",
    "\n",
    "resultt = evaluate_models(all_preds_t, targets_t)\n",
    "\n",
    "# View metrics\n",
    "print(f\"***************Individual Model Metrics********************************\")\n",
    "for m in resultt['individual_metrics']:\n",
    "    print(f\"Model {m['model_idx']+1} → R2: {m['r2']:.3f}, RMSE: {m['rmse']:.3f}, MAE: {m['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n",
    "\n",
    "print(f\"\\nBest Model : Model {resultt['best_model_index']+1}\")\n",
    "print(f\"***********************************************************************\")\n",
    "print(f\"Ensemble R2: {resultt['ensemble_metrics']['r2']:.3f}\")\n",
    "print(f\"Ensemble RMSE: {resultt['ensemble_metrics']['rmse']:.3f}\")\n",
    "print(f\"Ensemble MAE: {resultt['ensemble_metrics']['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad73209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/5 with seed 42\n",
      "Epoch 1/100 | Train Loss: 83281.6300 | Val Loss: 150.0876 | R²: -0.1295 | RMSE: 63856.8398\n",
      "Epoch 2/100 | Train Loss: 88608.0228 | Val Loss: 150.5495 | R²: -0.1151 | RMSE: 63044.9141\n",
      "Epoch 3/100 | Train Loss: 80431.7794 | Val Loss: 150.4671 | R²: -0.1075 | RMSE: 62612.7695\n",
      "Epoch 4/100 | Train Loss: 88540.6658 | Val Loss: 151.4975 | R²: -0.0783 | RMSE: 60961.1875\n",
      "Epoch 5/100 | Train Loss: 83023.4114 | Val Loss: 152.9523 | R²: -0.0520 | RMSE: 59476.3945\n",
      "Epoch 6/100 | Train Loss: 82167.3363 | Val Loss: 154.4995 | R²: -0.0444 | RMSE: 59047.3242\n",
      "Epoch 7/100 | Train Loss: 83034.7643 | Val Loss: 154.2074 | R²: -0.0396 | RMSE: 58776.6367\n",
      "Epoch 8/100 | Train Loss: 80734.0245 | Val Loss: 158.7559 | R²: -0.0073 | RMSE: 56949.9727\n",
      "Epoch 9/100 | Train Loss: 78076.3537 | Val Loss: 164.0019 | R²: 0.0097 | RMSE: 55988.1602\n",
      "Epoch 10/100 | Train Loss: 78460.5496 | Val Loss: 159.6242 | R²: -0.0007 | RMSE: 56573.7031\n",
      "Epoch 11/100 | Train Loss: 80916.2811 | Val Loss: 162.4797 | R²: 0.0052 | RMSE: 56238.7266\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training model 2/5 with seed 52\n",
      "Epoch 1/100 | Train Loss: 84380.8735 | Val Loss: 151.4309 | R²: -0.1380 | RMSE: 64434.8711\n",
      "Epoch 2/100 | Train Loss: 84019.2600 | Val Loss: 151.1786 | R²: -0.1312 | RMSE: 64051.3047\n",
      "Epoch 3/100 | Train Loss: 83271.9397 | Val Loss: 151.7174 | R²: -0.0918 | RMSE: 61817.2891\n",
      "Epoch 4/100 | Train Loss: 84444.7970 | Val Loss: 171.1735 | R²: -1.1422 | RMSE: 121290.3984\n",
      "Epoch 5/100 | Train Loss: 86468.5350 | Val Loss: 151.8703 | R²: -0.0773 | RMSE: 60994.7188\n",
      "Epoch 6/100 | Train Loss: 84755.3014 | Val Loss: 151.9052 | R²: -0.0659 | RMSE: 60354.3281\n",
      "Epoch 7/100 | Train Loss: 87286.4940 | Val Loss: 153.5180 | R²: -0.0413 | RMSE: 58959.1797\n",
      "Epoch 8/100 | Train Loss: 80261.7842 | Val Loss: 153.9632 | R²: -0.0329 | RMSE: 58485.7891\n",
      "Epoch 9/100 | Train Loss: 78632.7338 | Val Loss: 157.5215 | R²: -0.0113 | RMSE: 57262.1641\n",
      "Epoch 10/100 | Train Loss: 79947.4694 | Val Loss: 165.4004 | R²: 0.0112 | RMSE: 55987.7891\n",
      "Epoch 11/100 | Train Loss: 78921.7424 | Val Loss: 161.3264 | R²: 0.0027 | RMSE: 56467.6758\n",
      "Epoch 12/100 | Train Loss: 75756.7443 | Val Loss: 165.6369 | R²: 0.0098 | RMSE: 56067.0430\n",
      "Early stopping at epoch 12\n",
      "\n",
      "Training model 3/5 with seed 62\n",
      "Epoch 1/100 | Train Loss: 89066.1966 | Val Loss: 150.6684 | R²: -0.1220 | RMSE: 62981.3320\n",
      "Epoch 2/100 | Train Loss: 81832.2156 | Val Loss: 149.9428 | R²: -0.1046 | RMSE: 62006.5508\n",
      "Epoch 3/100 | Train Loss: 85342.6459 | Val Loss: 151.6046 | R²: -0.0683 | RMSE: 59969.0352\n",
      "Epoch 4/100 | Train Loss: 84070.9383 | Val Loss: 150.9023 | R²: -0.0817 | RMSE: 60719.0859\n",
      "Epoch 5/100 | Train Loss: 83261.6812 | Val Loss: 154.4382 | R²: -0.0201 | RMSE: 57261.5508\n",
      "Epoch 6/100 | Train Loss: 89369.3032 | Val Loss: 192.1496 | R²: -1.7374 | RMSE: 153664.0312\n",
      "Epoch 7/100 | Train Loss: 80468.9788 | Val Loss: 162.4638 | R²: 0.0036 | RMSE: 55931.4844\n",
      "Epoch 8/100 | Train Loss: 82248.4800 | Val Loss: 162.7322 | R²: 0.0173 | RMSE: 55160.9844\n",
      "Epoch 9/100 | Train Loss: 78358.2397 | Val Loss: 159.9996 | R²: 0.0219 | RMSE: 54905.5742\n",
      "Epoch 10/100 | Train Loss: 83561.3288 | Val Loss: 159.0804 | R²: -0.0034 | RMSE: 56325.8242\n",
      "Epoch 11/100 | Train Loss: 82031.6738 | Val Loss: 164.5645 | R²: 0.0377 | RMSE: 54020.9609\n",
      "Epoch 12/100 | Train Loss: 81290.8958 | Val Loss: 161.2331 | R²: 0.0208 | RMSE: 54966.6445\n",
      "Early stopping at epoch 12\n",
      "\n",
      "Training model 4/5 with seed 72\n",
      "Epoch 1/100 | Train Loss: 85643.1324 | Val Loss: 150.4164 | R²: -0.1213 | RMSE: 63037.7969\n",
      "Epoch 2/100 | Train Loss: 92350.1273 | Val Loss: 149.8579 | R²: -0.1238 | RMSE: 63180.9258\n",
      "Epoch 3/100 | Train Loss: 87979.0055 | Val Loss: 152.7867 | R²: -0.0677 | RMSE: 60028.0156\n",
      "Epoch 4/100 | Train Loss: 89630.7237 | Val Loss: 155.4536 | R²: -0.0259 | RMSE: 57675.3828\n",
      "Epoch 5/100 | Train Loss: 80611.0511 | Val Loss: 150.0849 | R²: -0.1110 | RMSE: 62460.6250\n",
      "Epoch 6/100 | Train Loss: 84224.1578 | Val Loss: 152.2737 | R²: -0.1093 | RMSE: 62366.1680\n",
      "Epoch 7/100 | Train Loss: 82649.7297 | Val Loss: 151.6001 | R²: -0.1079 | RMSE: 62285.1016\n",
      "Epoch 8/100 | Train Loss: 84586.3524 | Val Loss: 171.3159 | R²: -0.0775 | RMSE: 60578.0977\n",
      "Epoch 9/100 | Train Loss: 79281.8192 | Val Loss: 154.3014 | R²: -0.0725 | RMSE: 60296.9883\n",
      "Epoch 10/100 | Train Loss: 78418.0722 | Val Loss: 154.0500 | R²: -0.0446 | RMSE: 58724.6523\n",
      "Epoch 11/100 | Train Loss: 83585.4863 | Val Loss: 172.3933 | R²: 0.0116 | RMSE: 55564.7539\n",
      "Epoch 12/100 | Train Loss: 82338.3696 | Val Loss: 218.8515 | R²: -0.1640 | RMSE: 65442.1445\n",
      "Early stopping at epoch 12\n",
      "\n",
      "Training model 5/5 with seed 82\n",
      "Epoch 1/100 | Train Loss: 89735.1521 | Val Loss: 152.0883 | R²: -0.1310 | RMSE: 63920.1016\n",
      "Epoch 2/100 | Train Loss: 85609.8875 | Val Loss: 151.5402 | R²: -0.1228 | RMSE: 63454.9766\n",
      "Epoch 3/100 | Train Loss: 85703.8330 | Val Loss: 157.7412 | R²: -0.0357 | RMSE: 58533.3242\n",
      "Epoch 4/100 | Train Loss: 89772.1346 | Val Loss: 179.8079 | R²: -1.6003 | RMSE: 146957.7656\n",
      "Epoch 5/100 | Train Loss: 87230.3660 | Val Loss: 157.1960 | R²: -0.0435 | RMSE: 58975.6094\n",
      "Epoch 6/100 | Train Loss: 82437.9526 | Val Loss: 158.4034 | R²: -0.1194 | RMSE: 63261.2891\n",
      "Epoch 7/100 | Train Loss: 83624.7019 | Val Loss: 172.6294 | R²: 0.0085 | RMSE: 56034.3398\n",
      "Epoch 8/100 | Train Loss: 84239.7070 | Val Loss: 167.5036 | R²: 0.0038 | RMSE: 56299.9844\n",
      "Epoch 9/100 | Train Loss: 78643.5957 | Val Loss: 168.7023 | R²: -0.3063 | RMSE: 73828.5234\n",
      "Epoch 10/100 | Train Loss: 77305.6870 | Val Loss: 160.7117 | R²: -0.0028 | RMSE: 56672.8320\n",
      "Epoch 11/100 | Train Loss: 80475.8618 | Val Loss: 171.1591 | R²: -0.2872 | RMSE: 72745.5781\n",
      "Epoch 12/100 | Train Loss: 77390.5958 | Val Loss: 160.1025 | R²: -0.0202 | RMSE: 57655.1797\n",
      "Early stopping at epoch 12\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Train ensemble models without embeddings\n",
    "##########################################\n",
    "\n",
    "models_e = train_ensemble(num_models=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5cb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Get predictions and targets\n",
    "################################################\n",
    "\n",
    "_, val_loader_e = get_data_loaders(seed=42, batch_size=16)\n",
    "all_preds_e, targets_e = get_ensemble_predictions(models_e, val_loader_e, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Individual Model Metrics********************************\n",
      "Model 1 → R2: 0.006, RMSE: 237.097, MAE: 163.837\n",
      "Model 2 → R2: 0.016, RMSE: 235.908, MAE: 166.959\n",
      "Model 3 → R2: 0.012, RMSE: 236.353, MAE: 164.456\n",
      "Model 4 → R2: -0.161, RMSE: 256.172, MAE: 220.691\n",
      "Model 5 → R2: 0.003, RMSE: 237.374, MAE: 160.566\n",
      "***********************************************************************\n",
      "\n",
      "Best Model : Model 2\n",
      "***********************************************************************\n",
      "Ensemble R2: 0.021\n",
      "Ensemble RMSE: 235.216\n",
      "Ensemble MAE: 172.722\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Evaluate the ensemble models for our CAPRI-CT without embeddings\n",
    "#####################################################################\n",
    "\n",
    "resulte = evaluate_models(all_preds_e, targets_e)\n",
    "\n",
    "# View metrics\n",
    "print(f\"***************Individual Model Metrics********************************\")\n",
    "for m in resulte['individual_metrics']:\n",
    "    print(f\"Model {m['model_idx']+1} → R2: {m['r2']:.3f}, RMSE: {m['rmse']:.3f}, MAE: {m['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n",
    "\n",
    "print(f\"\\nBest Model : Model {resulte['best_model_index']+1}\")\n",
    "print(f\"***********************************************************************\")\n",
    "print(f\"Ensemble R2: {resulte['ensemble_metrics']['r2']:.3f}\")\n",
    "print(f\"Ensemble RMSE: {resulte['ensemble_metrics']['rmse']:.3f}\")\n",
    "print(f\"Ensemble MAE: {resulte['ensemble_metrics']['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/5 with seed 42\n",
      "Epoch 1/100 | Train Loss: 84864.0635 | Val Loss: 145.4918 | R²: -0.0662 | RMSE: 60275.0703\n",
      "Epoch 2/100 | Train Loss: 78565.0575 | Val Loss: 143.2604 | R²: -0.0278 | RMSE: 58108.3125\n",
      "Epoch 3/100 | Train Loss: 77237.8451 | Val Loss: 138.8447 | R²: 0.0409 | RMSE: 54222.1953\n",
      "Epoch 4/100 | Train Loss: 76129.1559 | Val Loss: 132.6336 | R²: 0.1370 | RMSE: 48790.2109\n",
      "Epoch 5/100 | Train Loss: 66365.2471 | Val Loss: 126.6488 | R²: 0.2203 | RMSE: 44082.2031\n",
      "Epoch 6/100 | Train Loss: 55886.1469 | Val Loss: 117.9415 | R²: 0.3399 | RMSE: 37318.7656\n",
      "Epoch 7/100 | Train Loss: 55338.3519 | Val Loss: 116.4466 | R²: 0.3636 | RMSE: 35979.0781\n",
      "Epoch 8/100 | Train Loss: 44526.1115 | Val Loss: 106.1938 | R²: 0.5208 | RMSE: 27089.4355\n",
      "Epoch 9/100 | Train Loss: 38197.5289 | Val Loss: 97.4817 | R²: 0.5810 | RMSE: 23689.0469\n",
      "Epoch 10/100 | Train Loss: 35930.1165 | Val Loss: 101.6316 | R²: 0.5523 | RMSE: 25313.4863\n",
      "Epoch 11/100 | Train Loss: 30967.9114 | Val Loss: 139.1190 | R²: 0.3436 | RMSE: 37110.7812\n",
      "Epoch 12/100 | Train Loss: 30087.2166 | Val Loss: 89.3025 | R²: 0.6728 | RMSE: 18499.6602\n",
      "Epoch 13/100 | Train Loss: 27752.0323 | Val Loss: 113.6265 | R²: 0.5616 | RMSE: 24787.4453\n",
      "Epoch 14/100 | Train Loss: 25514.7063 | Val Loss: 104.9361 | R²: 0.6055 | RMSE: 22304.6133\n",
      "Epoch 15/100 | Train Loss: 23039.2037 | Val Loss: 117.1083 | R²: 0.5252 | RMSE: 26841.1855\n",
      "Epoch 16/100 | Train Loss: 22099.8948 | Val Loss: 76.2876 | R²: 0.7756 | RMSE: 12683.9512\n",
      "Epoch 17/100 | Train Loss: 21630.6826 | Val Loss: 75.7803 | R²: 0.7697 | RMSE: 13019.0303\n",
      "Epoch 18/100 | Train Loss: 23645.4565 | Val Loss: 83.8501 | R²: 0.7431 | RMSE: 14524.1973\n",
      "Epoch 19/100 | Train Loss: 22390.3216 | Val Loss: 77.2228 | R²: 0.7736 | RMSE: 12798.0869\n",
      "Epoch 20/100 | Train Loss: 20667.4398 | Val Loss: 72.9371 | R²: 0.7790 | RMSE: 12496.6826\n",
      "Epoch 21/100 | Train Loss: 21532.9802 | Val Loss: 75.0391 | R²: 0.7804 | RMSE: 12413.9805\n",
      "Epoch 22/100 | Train Loss: 20748.8624 | Val Loss: 73.9155 | R²: 0.7805 | RMSE: 12411.3691\n",
      "Epoch 23/100 | Train Loss: 17855.6737 | Val Loss: 82.6860 | R²: 0.7560 | RMSE: 13792.4062\n",
      "Epoch 24/100 | Train Loss: 18785.5853 | Val Loss: 93.7494 | R²: 0.6332 | RMSE: 20734.8477\n",
      "Epoch 25/100 | Train Loss: 21193.0263 | Val Loss: 97.5772 | R²: 0.6125 | RMSE: 21905.8770\n",
      "Epoch 26/100 | Train Loss: 19329.1150 | Val Loss: 101.5482 | R²: 0.6373 | RMSE: 20504.9121\n",
      "Epoch 27/100 | Train Loss: 20883.1584 | Val Loss: 70.7025 | R²: 0.7863 | RMSE: 12082.9834\n",
      "Epoch 28/100 | Train Loss: 19543.1935 | Val Loss: 70.3693 | R²: 0.7840 | RMSE: 12209.6387\n",
      "Epoch 29/100 | Train Loss: 18759.1548 | Val Loss: 70.8643 | R²: 0.7913 | RMSE: 11798.0059\n",
      "Epoch 30/100 | Train Loss: 18988.7014 | Val Loss: 70.2383 | R²: 0.7900 | RMSE: 11875.2236\n",
      "Epoch 31/100 | Train Loss: 19834.3813 | Val Loss: 69.4297 | R²: 0.7871 | RMSE: 12035.9258\n",
      "Epoch 32/100 | Train Loss: 20552.2694 | Val Loss: 72.2954 | R²: 0.7664 | RMSE: 13208.5537\n",
      "Epoch 33/100 | Train Loss: 19202.2542 | Val Loss: 78.6917 | R²: 0.7650 | RMSE: 13288.4619\n",
      "Epoch 34/100 | Train Loss: 19401.5154 | Val Loss: 74.1870 | R²: 0.7849 | RMSE: 12158.6162\n",
      "Epoch 35/100 | Train Loss: 18945.6108 | Val Loss: 69.8061 | R²: 0.7906 | RMSE: 11840.7510\n",
      "Epoch 36/100 | Train Loss: 20082.1362 | Val Loss: 94.3337 | R²: 0.6343 | RMSE: 20675.1523\n",
      "Epoch 37/100 | Train Loss: 19174.8735 | Val Loss: 71.2693 | R²: 0.7868 | RMSE: 12055.4082\n",
      "Epoch 38/100 | Train Loss: 19005.2020 | Val Loss: 77.5195 | R²: 0.7465 | RMSE: 14333.7705\n",
      "Epoch 39/100 | Train Loss: 20251.2205 | Val Loss: 71.3232 | R²: 0.7919 | RMSE: 11765.4521\n",
      "Epoch 40/100 | Train Loss: 18906.9970 | Val Loss: 71.5536 | R²: 0.7930 | RMSE: 11703.2822\n",
      "Epoch 41/100 | Train Loss: 18098.5253 | Val Loss: 72.5187 | R²: 0.7863 | RMSE: 12080.8125\n",
      "Early stopping at epoch 41\n",
      "\n",
      "Training model 2/5 with seed 52\n",
      "Epoch 1/100 | Train Loss: 83606.7062 | Val Loss: 149.4424 | R²: -0.1072 | RMSE: 62687.2891\n",
      "Epoch 2/100 | Train Loss: 79492.2759 | Val Loss: 144.3878 | R²: -0.0092 | RMSE: 57140.7188\n",
      "Epoch 3/100 | Train Loss: 80242.9402 | Val Loss: 141.4065 | R²: 0.0453 | RMSE: 54057.7266\n",
      "Epoch 4/100 | Train Loss: 72866.7853 | Val Loss: 145.4494 | R²: 0.0245 | RMSE: 55231.6523\n",
      "Epoch 5/100 | Train Loss: 66047.9860 | Val Loss: 132.4588 | R²: 0.1788 | RMSE: 46494.1172\n",
      "Epoch 6/100 | Train Loss: 65077.0652 | Val Loss: 128.2859 | R²: 0.2559 | RMSE: 42129.1680\n",
      "Epoch 7/100 | Train Loss: 51946.7274 | Val Loss: 119.5746 | R²: 0.3652 | RMSE: 35944.8984\n",
      "Epoch 8/100 | Train Loss: 47943.0930 | Val Loss: 129.5811 | R²: 0.3489 | RMSE: 36865.9141\n",
      "Epoch 9/100 | Train Loss: 43004.5947 | Val Loss: 105.4576 | R²: 0.5355 | RMSE: 26299.8652\n",
      "Epoch 10/100 | Train Loss: 35965.1781 | Val Loss: 129.1811 | R²: 0.4218 | RMSE: 32738.7207\n",
      "Epoch 11/100 | Train Loss: 31158.0439 | Val Loss: 96.3772 | R²: 0.6301 | RMSE: 20941.1211\n",
      "Epoch 12/100 | Train Loss: 33029.3464 | Val Loss: 121.6636 | R²: 0.4928 | RMSE: 28716.5117\n",
      "Epoch 13/100 | Train Loss: 27609.3421 | Val Loss: 87.0159 | R²: 0.7149 | RMSE: 16142.2607\n",
      "Epoch 14/100 | Train Loss: 25233.2180 | Val Loss: 83.6084 | R²: 0.7378 | RMSE: 14848.3223\n",
      "Epoch 15/100 | Train Loss: 23560.1132 | Val Loss: 95.8990 | R²: 0.6638 | RMSE: 19034.8340\n",
      "Epoch 16/100 | Train Loss: 23244.3430 | Val Loss: 78.8101 | R²: 0.7538 | RMSE: 13940.8857\n",
      "Epoch 17/100 | Train Loss: 20869.5120 | Val Loss: 74.8864 | R²: 0.7797 | RMSE: 12472.9033\n",
      "Epoch 18/100 | Train Loss: 20797.2374 | Val Loss: 89.7989 | R²: 0.6990 | RMSE: 17042.4355\n",
      "Epoch 19/100 | Train Loss: 22305.9110 | Val Loss: 79.7133 | R²: 0.7704 | RMSE: 12999.7627\n",
      "Epoch 20/100 | Train Loss: 20620.9251 | Val Loss: 122.6390 | R²: 0.4052 | RMSE: 33679.5234\n",
      "Epoch 21/100 | Train Loss: 22185.6835 | Val Loss: 74.9099 | R²: 0.7848 | RMSE: 12186.6045\n",
      "Epoch 22/100 | Train Loss: 20443.8166 | Val Loss: 112.4448 | R²: 0.4581 | RMSE: 30685.0371\n",
      "Epoch 23/100 | Train Loss: 19379.9541 | Val Loss: 73.3856 | R²: 0.7913 | RMSE: 11818.5381\n",
      "Epoch 24/100 | Train Loss: 19188.0965 | Val Loss: 71.4927 | R²: 0.7918 | RMSE: 11785.6055\n",
      "Epoch 25/100 | Train Loss: 21199.7459 | Val Loss: 71.7590 | R²: 0.7903 | RMSE: 11875.5498\n",
      "Epoch 26/100 | Train Loss: 20505.9944 | Val Loss: 75.8969 | R²: 0.7828 | RMSE: 12297.5811\n",
      "Epoch 27/100 | Train Loss: 18909.7639 | Val Loss: 73.0401 | R²: 0.7872 | RMSE: 12051.0488\n",
      "Epoch 28/100 | Train Loss: 18785.2482 | Val Loss: 72.3525 | R²: 0.7851 | RMSE: 12169.6162\n",
      "Epoch 29/100 | Train Loss: 20067.8969 | Val Loss: 100.0227 | R²: 0.6003 | RMSE: 22630.8750\n",
      "Epoch 30/100 | Train Loss: 19441.9049 | Val Loss: 71.6962 | R²: 0.7886 | RMSE: 11971.1738\n",
      "Epoch 31/100 | Train Loss: 20291.1537 | Val Loss: 71.2861 | R²: 0.7919 | RMSE: 11782.9395\n",
      "Epoch 32/100 | Train Loss: 20440.4117 | Val Loss: 76.8639 | R²: 0.7811 | RMSE: 12392.3789\n",
      "Epoch 33/100 | Train Loss: 19358.6916 | Val Loss: 75.1386 | R²: 0.7828 | RMSE: 12299.5225\n",
      "Epoch 34/100 | Train Loss: 18402.1028 | Val Loss: 73.7627 | R²: 0.7834 | RMSE: 12263.3096\n",
      "Epoch 35/100 | Train Loss: 18998.2248 | Val Loss: 73.9185 | R²: 0.7864 | RMSE: 12092.5381\n",
      "Epoch 36/100 | Train Loss: 18009.8663 | Val Loss: 72.6341 | R²: 0.7870 | RMSE: 12061.9941\n",
      "Epoch 37/100 | Train Loss: 19146.1360 | Val Loss: 71.7023 | R²: 0.7926 | RMSE: 11740.3457\n",
      "Epoch 38/100 | Train Loss: 19130.6899 | Val Loss: 72.3400 | R²: 0.7939 | RMSE: 11670.0166\n",
      "Epoch 39/100 | Train Loss: 19448.8696 | Val Loss: 76.0203 | R²: 0.7808 | RMSE: 12409.4219\n",
      "Epoch 40/100 | Train Loss: 18013.4249 | Val Loss: 71.1563 | R²: 0.7928 | RMSE: 11730.1055\n",
      "Epoch 41/100 | Train Loss: 18571.8903 | Val Loss: 70.9537 | R²: 0.7944 | RMSE: 11639.7314\n",
      "Epoch 42/100 | Train Loss: 19087.7778 | Val Loss: 73.7728 | R²: 0.7820 | RMSE: 12344.8262\n",
      "Epoch 43/100 | Train Loss: 18217.5091 | Val Loss: 71.2945 | R²: 0.7915 | RMSE: 11805.3584\n",
      "Epoch 44/100 | Train Loss: 15957.4512 | Val Loss: 71.1938 | R²: 0.7923 | RMSE: 11760.6191\n",
      "Epoch 45/100 | Train Loss: 17421.9440 | Val Loss: 71.4535 | R²: 0.7894 | RMSE: 11926.8682\n",
      "Epoch 46/100 | Train Loss: 18797.0485 | Val Loss: 71.4804 | R²: 0.7929 | RMSE: 11726.0977\n",
      "Epoch 47/100 | Train Loss: 17315.3589 | Val Loss: 71.3562 | R²: 0.7896 | RMSE: 11913.7988\n",
      "Epoch 48/100 | Train Loss: 19437.7387 | Val Loss: 71.8788 | R²: 0.7923 | RMSE: 11758.7637\n",
      "Epoch 49/100 | Train Loss: 20409.2749 | Val Loss: 72.1522 | R²: 0.7897 | RMSE: 11907.7939\n",
      "Epoch 50/100 | Train Loss: 18757.1698 | Val Loss: 70.9775 | R²: 0.7925 | RMSE: 11750.4023\n",
      "Epoch 51/100 | Train Loss: 17807.3798 | Val Loss: 69.8006 | R²: 0.7990 | RMSE: 11378.9326\n",
      "Epoch 52/100 | Train Loss: 18761.5213 | Val Loss: 72.0254 | R²: 0.7909 | RMSE: 11841.7314\n",
      "Epoch 53/100 | Train Loss: 17174.9942 | Val Loss: 70.9904 | R²: 0.7945 | RMSE: 11637.6484\n",
      "Epoch 54/100 | Train Loss: 17731.5956 | Val Loss: 71.7575 | R²: 0.7876 | RMSE: 12027.3281\n",
      "Epoch 55/100 | Train Loss: 18790.2064 | Val Loss: 71.4571 | R²: 0.7928 | RMSE: 11733.4941\n",
      "Epoch 56/100 | Train Loss: 17193.9708 | Val Loss: 70.8810 | R²: 0.7960 | RMSE: 11552.8965\n",
      "Epoch 57/100 | Train Loss: 18708.8900 | Val Loss: 71.5055 | R²: 0.7935 | RMSE: 11693.5977\n",
      "Epoch 58/100 | Train Loss: 16683.5135 | Val Loss: 71.9574 | R²: 0.7916 | RMSE: 11798.4229\n",
      "Epoch 59/100 | Train Loss: 18526.2997 | Val Loss: 72.0575 | R²: 0.7942 | RMSE: 11654.1064\n",
      "Epoch 60/100 | Train Loss: 17019.9582 | Val Loss: 72.7857 | R²: 0.7919 | RMSE: 11784.8818\n",
      "Epoch 61/100 | Train Loss: 18136.6477 | Val Loss: 69.6792 | R²: 0.7956 | RMSE: 11573.1689\n",
      "Epoch 62/100 | Train Loss: 17424.7613 | Val Loss: 70.7609 | R²: 0.7963 | RMSE: 11531.0234\n",
      "Epoch 63/100 | Train Loss: 17656.9312 | Val Loss: 69.9643 | R²: 0.7967 | RMSE: 11511.1611\n",
      "Epoch 64/100 | Train Loss: 17007.3416 | Val Loss: 71.0530 | R²: 0.7896 | RMSE: 11912.2305\n",
      "Epoch 65/100 | Train Loss: 19941.6007 | Val Loss: 71.9633 | R²: 0.7909 | RMSE: 11839.5127\n",
      "Epoch 66/100 | Train Loss: 16600.0544 | Val Loss: 72.7540 | R²: 0.7920 | RMSE: 11777.3291\n",
      "Epoch 67/100 | Train Loss: 17651.4666 | Val Loss: 71.5664 | R²: 0.7922 | RMSE: 11766.1289\n",
      "Epoch 68/100 | Train Loss: 18241.6846 | Val Loss: 70.8370 | R²: 0.7900 | RMSE: 11888.3682\n",
      "Epoch 69/100 | Train Loss: 17723.2339 | Val Loss: 71.2131 | R²: 0.7943 | RMSE: 11647.1064\n",
      "Epoch 70/100 | Train Loss: 17143.2264 | Val Loss: 70.1788 | R²: 0.7957 | RMSE: 11566.3994\n",
      "Epoch 71/100 | Train Loss: 17078.8439 | Val Loss: 70.8431 | R²: 0.7933 | RMSE: 11700.7402\n",
      "Early stopping at epoch 71\n",
      "\n",
      "Training model 3/5 with seed 62\n",
      "Epoch 1/100 | Train Loss: 88708.6424 | Val Loss: 147.4925 | R²: -0.0867 | RMSE: 61003.5234\n",
      "Epoch 2/100 | Train Loss: 84991.7551 | Val Loss: 143.5090 | R²: -0.0197 | RMSE: 57241.2539\n",
      "Epoch 3/100 | Train Loss: 76695.6060 | Val Loss: 137.8991 | R²: 0.0639 | RMSE: 52548.4336\n",
      "Epoch 4/100 | Train Loss: 71114.5330 | Val Loss: 135.0324 | R²: 0.1402 | RMSE: 48264.8242\n",
      "Epoch 5/100 | Train Loss: 63787.5871 | Val Loss: 127.4946 | R²: 0.2157 | RMSE: 44027.4062\n",
      "Epoch 6/100 | Train Loss: 57707.7460 | Val Loss: 128.9474 | R²: 0.2413 | RMSE: 42589.7188\n",
      "Epoch 7/100 | Train Loss: 54516.4098 | Val Loss: 112.2363 | R²: 0.4220 | RMSE: 32444.3008\n",
      "Epoch 8/100 | Train Loss: 48932.1987 | Val Loss: 142.4433 | R²: -0.4753 | RMSE: 82817.7344\n",
      "Epoch 9/100 | Train Loss: 44547.6519 | Val Loss: 133.7303 | R²: 0.3442 | RMSE: 36812.0781\n",
      "Epoch 10/100 | Train Loss: 37044.8994 | Val Loss: 845.0851 | R²: -1523.5639 | RMSE: 85580712.0000\n",
      "Epoch 11/100 | Train Loss: 32744.8447 | Val Loss: 91.9527 | R²: 0.6469 | RMSE: 19818.9785\n",
      "Epoch 12/100 | Train Loss: 32084.3883 | Val Loss: 105.0518 | R²: 0.5967 | RMSE: 22638.8301\n",
      "Epoch 13/100 | Train Loss: 27520.7586 | Val Loss: 84.5495 | R²: 0.7062 | RMSE: 16494.4238\n",
      "Epoch 14/100 | Train Loss: 25790.9811 | Val Loss: 88.3643 | R²: 0.7084 | RMSE: 16370.3066\n",
      "Epoch 15/100 | Train Loss: 23029.9141 | Val Loss: 83.7742 | R²: 0.7143 | RMSE: 16039.4238\n",
      "Epoch 16/100 | Train Loss: 21291.9379 | Val Loss: 79.6261 | R²: 0.7586 | RMSE: 13553.6367\n",
      "Epoch 17/100 | Train Loss: 21125.9288 | Val Loss: 72.8830 | R²: 0.7919 | RMSE: 11680.9814\n",
      "Epoch 18/100 | Train Loss: 20910.7500 | Val Loss: 72.7923 | R²: 0.7804 | RMSE: 12329.8682\n",
      "Epoch 19/100 | Train Loss: 20803.5075 | Val Loss: 73.7586 | R²: 0.7862 | RMSE: 11999.9785\n",
      "Epoch 20/100 | Train Loss: 21935.0099 | Val Loss: 73.6674 | R²: 0.7900 | RMSE: 11789.7490\n",
      "Epoch 21/100 | Train Loss: 21005.6548 | Val Loss: 88.6229 | R²: 0.7393 | RMSE: 14633.4189\n",
      "Epoch 22/100 | Train Loss: 20957.5565 | Val Loss: 76.7636 | R²: 0.7542 | RMSE: 13797.7305\n",
      "Epoch 23/100 | Train Loss: 18812.7073 | Val Loss: 79.6490 | R²: 0.7694 | RMSE: 12945.1562\n",
      "Epoch 24/100 | Train Loss: 19991.9139 | Val Loss: 82.6706 | R²: 0.7587 | RMSE: 13547.9561\n",
      "Epoch 25/100 | Train Loss: 21631.4029 | Val Loss: 118.6464 | R²: 0.4684 | RMSE: 29842.7539\n",
      "Epoch 26/100 | Train Loss: 20098.1522 | Val Loss: 75.4544 | R²: 0.7793 | RMSE: 12386.1719\n",
      "Epoch 27/100 | Train Loss: 18553.5594 | Val Loss: 71.4581 | R²: 0.7903 | RMSE: 11773.1846\n",
      "Epoch 28/100 | Train Loss: 20294.0520 | Val Loss: 70.0108 | R²: 0.7951 | RMSE: 11501.4902\n",
      "Epoch 29/100 | Train Loss: 20310.5839 | Val Loss: 71.5896 | R²: 0.7858 | RMSE: 12026.1201\n",
      "Epoch 30/100 | Train Loss: 20214.3021 | Val Loss: 82.1115 | R²: 0.7121 | RMSE: 16160.5254\n",
      "Epoch 31/100 | Train Loss: 17384.9124 | Val Loss: 83.1993 | R²: 0.7072 | RMSE: 16436.5449\n",
      "Epoch 32/100 | Train Loss: 19213.2761 | Val Loss: 67.9462 | R²: 0.7960 | RMSE: 11452.1953\n",
      "Epoch 33/100 | Train Loss: 19486.5165 | Val Loss: 73.3571 | R²: 0.7884 | RMSE: 11877.4609\n",
      "Epoch 34/100 | Train Loss: 19108.8614 | Val Loss: 67.9229 | R²: 0.8018 | RMSE: 11124.2549\n",
      "Epoch 35/100 | Train Loss: 17548.7614 | Val Loss: 69.4528 | R²: 0.7912 | RMSE: 11719.2461\n",
      "Epoch 36/100 | Train Loss: 19342.3436 | Val Loss: 72.0895 | R²: 0.7909 | RMSE: 11738.0049\n",
      "Epoch 37/100 | Train Loss: 18148.6259 | Val Loss: 69.9303 | R²: 0.7910 | RMSE: 11730.2207\n",
      "Epoch 38/100 | Train Loss: 18699.9991 | Val Loss: 73.3997 | R²: 0.7637 | RMSE: 13264.9346\n",
      "Epoch 39/100 | Train Loss: 20248.0067 | Val Loss: 70.1932 | R²: 0.7874 | RMSE: 11932.1855\n",
      "Epoch 40/100 | Train Loss: 18789.2339 | Val Loss: 73.8153 | R²: 0.7638 | RMSE: 13259.7227\n",
      "Epoch 41/100 | Train Loss: 18957.1375 | Val Loss: 68.2784 | R²: 0.7971 | RMSE: 11388.4873\n",
      "Epoch 42/100 | Train Loss: 18079.3179 | Val Loss: 70.2166 | R²: 0.7887 | RMSE: 11861.6807\n",
      "Epoch 43/100 | Train Loss: 18158.2938 | Val Loss: 68.2749 | R²: 0.7957 | RMSE: 11468.6748\n",
      "Epoch 44/100 | Train Loss: 17338.7746 | Val Loss: 69.9982 | R²: 0.7929 | RMSE: 11625.8711\n",
      "Early stopping at epoch 44\n",
      "\n",
      "Training model 4/5 with seed 72\n",
      "Epoch 1/100 | Train Loss: 88577.2960 | Val Loss: 147.1210 | R²: -0.0816 | RMSE: 60808.9258\n",
      "Epoch 2/100 | Train Loss: 82963.9733 | Val Loss: 144.3204 | R²: -0.0263 | RMSE: 57696.8672\n",
      "Epoch 3/100 | Train Loss: 77641.0595 | Val Loss: 140.3297 | R²: 0.0258 | RMSE: 54770.7383\n",
      "Epoch 4/100 | Train Loss: 69239.3483 | Val Loss: 136.0093 | R²: 0.1377 | RMSE: 48480.3203\n",
      "Epoch 5/100 | Train Loss: 66526.9702 | Val Loss: 135.6458 | R²: 0.1309 | RMSE: 48861.9531\n",
      "Epoch 6/100 | Train Loss: 65134.5246 | Val Loss: 282.9895 | R²: -68.8831 | RMSE: 3928799.5000\n",
      "Epoch 7/100 | Train Loss: 54833.8215 | Val Loss: 131.5431 | R²: 0.2205 | RMSE: 43824.4062\n",
      "Epoch 8/100 | Train Loss: 51136.3582 | Val Loss: 114.7636 | R²: 0.4097 | RMSE: 33186.9297\n",
      "Epoch 9/100 | Train Loss: 46929.9631 | Val Loss: 123.4285 | R²: 0.3789 | RMSE: 34915.9922\n",
      "Epoch 10/100 | Train Loss: 42577.2501 | Val Loss: 113.6555 | R²: 0.4629 | RMSE: 30197.1875\n",
      "Epoch 11/100 | Train Loss: 38320.8037 | Val Loss: 100.5871 | R²: 0.5835 | RMSE: 23412.7324\n",
      "Epoch 12/100 | Train Loss: 31654.4496 | Val Loss: 94.6467 | R²: 0.6647 | RMSE: 18849.9531\n",
      "Epoch 13/100 | Train Loss: 27825.1465 | Val Loss: 113.0708 | R²: 0.5242 | RMSE: 26747.1270\n",
      "Epoch 14/100 | Train Loss: 24036.0622 | Val Loss: 87.6809 | R²: 0.6975 | RMSE: 17004.6602\n",
      "Epoch 15/100 | Train Loss: 25253.7503 | Val Loss: 88.4766 | R²: 0.6916 | RMSE: 17336.1992\n",
      "Epoch 16/100 | Train Loss: 21795.8634 | Val Loss: 89.8281 | R²: 0.6994 | RMSE: 16897.7969\n",
      "Epoch 17/100 | Train Loss: 22260.7220 | Val Loss: 80.4685 | R²: 0.7319 | RMSE: 15070.8662\n",
      "Epoch 18/100 | Train Loss: 21814.7130 | Val Loss: 89.0543 | R²: 0.7051 | RMSE: 16576.8086\n",
      "Epoch 19/100 | Train Loss: 20770.2286 | Val Loss: 74.4512 | R²: 0.7702 | RMSE: 12920.8271\n",
      "Epoch 20/100 | Train Loss: 20654.6836 | Val Loss: 88.8924 | R²: 0.6901 | RMSE: 17424.9492\n",
      "Epoch 21/100 | Train Loss: 20360.7633 | Val Loss: 90.4856 | R²: 0.6881 | RMSE: 17534.8555\n",
      "Epoch 22/100 | Train Loss: 19244.8876 | Val Loss: 86.1941 | R²: 0.6965 | RMSE: 17063.5723\n",
      "Epoch 23/100 | Train Loss: 20983.8795 | Val Loss: 78.6561 | R²: 0.7452 | RMSE: 14324.4893\n",
      "Epoch 24/100 | Train Loss: 19243.4681 | Val Loss: 80.7906 | R²: 0.7398 | RMSE: 14626.3115\n",
      "Epoch 25/100 | Train Loss: 18765.5572 | Val Loss: 72.6737 | R²: 0.7677 | RMSE: 13060.4053\n",
      "Epoch 26/100 | Train Loss: 19263.7129 | Val Loss: 76.7675 | R²: 0.7603 | RMSE: 13474.7041\n",
      "Epoch 27/100 | Train Loss: 19035.8628 | Val Loss: 84.3080 | R²: 0.7237 | RMSE: 15531.3555\n",
      "Epoch 28/100 | Train Loss: 19100.8475 | Val Loss: 74.2120 | R²: 0.7691 | RMSE: 12980.9482\n",
      "Epoch 29/100 | Train Loss: 18686.3417 | Val Loss: 73.6921 | R²: 0.7710 | RMSE: 12875.5742\n",
      "Epoch 30/100 | Train Loss: 21037.7656 | Val Loss: 74.4679 | R²: 0.7597 | RMSE: 13509.1875\n",
      "Epoch 31/100 | Train Loss: 18858.8023 | Val Loss: 74.4060 | R²: 0.7632 | RMSE: 13311.8320\n",
      "Epoch 32/100 | Train Loss: 19459.3333 | Val Loss: 82.9180 | R²: 0.7077 | RMSE: 16432.1309\n",
      "Epoch 33/100 | Train Loss: 18108.9385 | Val Loss: 74.0737 | R²: 0.7541 | RMSE: 13825.3564\n",
      "Epoch 34/100 | Train Loss: 18489.5561 | Val Loss: 89.2426 | R²: 0.6824 | RMSE: 17856.0527\n",
      "Epoch 35/100 | Train Loss: 17548.3226 | Val Loss: 74.4478 | R²: 0.7614 | RMSE: 13412.5625\n",
      "Early stopping at epoch 35\n",
      "\n",
      "Training model 5/5 with seed 82\n",
      "Epoch 1/100 | Train Loss: 89094.2395 | Val Loss: 147.8842 | R²: -0.0816 | RMSE: 61128.0859\n",
      "Epoch 2/100 | Train Loss: 86929.0618 | Val Loss: 144.0286 | R²: -0.0241 | RMSE: 57875.5000\n",
      "Epoch 3/100 | Train Loss: 73557.6853 | Val Loss: 141.3943 | R²: 0.0256 | RMSE: 55068.7109\n",
      "Epoch 4/100 | Train Loss: 70925.2529 | Val Loss: 136.1014 | R²: 0.1009 | RMSE: 50814.2617\n",
      "Epoch 5/100 | Train Loss: 71690.9389 | Val Loss: 127.9223 | R²: 0.2186 | RMSE: 44161.1133\n",
      "Epoch 6/100 | Train Loss: 63525.3001 | Val Loss: 122.0799 | R²: 0.2987 | RMSE: 39637.2344\n",
      "Epoch 7/100 | Train Loss: 48739.3689 | Val Loss: 117.2470 | R²: 0.3828 | RMSE: 34882.4336\n",
      "Epoch 8/100 | Train Loss: 50694.0191 | Val Loss: 109.1020 | R²: 0.4837 | RMSE: 29177.8555\n",
      "Epoch 9/100 | Train Loss: 43670.7238 | Val Loss: 112.6706 | R²: 0.4373 | RMSE: 31803.9062\n",
      "Epoch 10/100 | Train Loss: 40199.7790 | Val Loss: 121.6707 | R²: -0.5425 | RMSE: 87178.1875\n",
      "Epoch 11/100 | Train Loss: 36492.6179 | Val Loss: 95.5715 | R²: 0.6160 | RMSE: 21702.9688\n",
      "Epoch 12/100 | Train Loss: 33362.5354 | Val Loss: 112.0943 | R²: 0.5096 | RMSE: 27715.1367\n",
      "Epoch 13/100 | Train Loss: 28516.6519 | Val Loss: 83.2891 | R²: 0.7289 | RMSE: 15322.2969\n",
      "Epoch 14/100 | Train Loss: 24469.3261 | Val Loss: 78.9302 | R²: 0.7541 | RMSE: 13894.8184\n",
      "Epoch 15/100 | Train Loss: 23837.1227 | Val Loss: 95.7321 | R²: 0.6614 | RMSE: 19136.0938\n",
      "Epoch 16/100 | Train Loss: 22432.7861 | Val Loss: 77.6689 | R²: 0.7686 | RMSE: 13077.7207\n",
      "Epoch 17/100 | Train Loss: 23331.1698 | Val Loss: 90.9729 | R²: 0.6925 | RMSE: 17379.3301\n",
      "Epoch 18/100 | Train Loss: 21257.2034 | Val Loss: 72.5321 | R²: 0.7859 | RMSE: 12098.2559\n",
      "Epoch 19/100 | Train Loss: 21863.4197 | Val Loss: 79.8957 | R²: 0.7517 | RMSE: 14032.1445\n",
      "Epoch 20/100 | Train Loss: 20162.0456 | Val Loss: 83.9537 | R²: 0.6646 | RMSE: 18955.2324\n",
      "Epoch 21/100 | Train Loss: 21020.1722 | Val Loss: 79.0184 | R²: 0.7354 | RMSE: 14952.0791\n",
      "Epoch 22/100 | Train Loss: 20002.9637 | Val Loss: 74.0615 | R²: 0.7770 | RMSE: 12604.1416\n",
      "Epoch 23/100 | Train Loss: 20353.4134 | Val Loss: 67.5154 | R²: 0.7917 | RMSE: 11770.0977\n",
      "Epoch 24/100 | Train Loss: 20777.3741 | Val Loss: 69.8614 | R²: 0.7900 | RMSE: 11867.3047\n",
      "Epoch 25/100 | Train Loss: 20084.5048 | Val Loss: 67.9225 | R²: 0.7829 | RMSE: 12268.0938\n",
      "Epoch 26/100 | Train Loss: 19846.4527 | Val Loss: 72.7377 | R²: 0.7747 | RMSE: 12731.3428\n",
      "Epoch 27/100 | Train Loss: 22427.8050 | Val Loss: 80.9046 | R²: 0.7284 | RMSE: 15351.8809\n",
      "Epoch 28/100 | Train Loss: 20125.6212 | Val Loss: 91.2695 | R²: 0.7133 | RMSE: 16204.7695\n",
      "Epoch 29/100 | Train Loss: 20365.9464 | Val Loss: 70.1845 | R²: 0.7830 | RMSE: 12263.5869\n",
      "Epoch 30/100 | Train Loss: 18779.6006 | Val Loss: 74.0268 | R²: 0.7495 | RMSE: 14158.6328\n",
      "Epoch 31/100 | Train Loss: 18913.5258 | Val Loss: 68.3423 | R²: 0.7910 | RMSE: 11812.2510\n",
      "Epoch 32/100 | Train Loss: 18505.7486 | Val Loss: 68.0155 | R²: 0.7999 | RMSE: 11310.1855\n",
      "Epoch 33/100 | Train Loss: 18108.5798 | Val Loss: 74.0368 | R²: 0.7564 | RMSE: 13768.2852\n",
      "Early stopping at epoch 33\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Train ensemble models with added noise\n",
    "##########################################\n",
    "\n",
    "models_r = train_ensemble(num_models=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Get predictions and targets\n",
    "################################################\n",
    "_, val_loader_r = get_data_loaders(seed=42, batch_size=16)\n",
    "all_preds_r, targets_r = get_ensemble_predictions(models_r, val_loader_r, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Individual Model Metrics********************************\n",
      "Model 1 → R2: 0.783, RMSE: 110.673, MAE: 72.906\n",
      "Model 2 → R2: 0.798, RMSE: 106.981, MAE: 68.327\n",
      "Model 3 → R2: 0.794, RMSE: 108.044, MAE: 69.872\n",
      "Model 4 → R2: 0.790, RMSE: 108.855, MAE: 70.707\n",
      "Model 5 → R2: 0.752, RMSE: 118.435, MAE: 76.341\n",
      "***********************************************************************\n",
      "\n",
      "Best Model : Model 2\n",
      "***********************************************************************\n",
      "Ensemble R2: 0.797\n",
      "Ensemble RMSE: 107.192\n",
      "Ensemble MAE: 68.704\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Evaluate the ensemble models for our CAPRI-CT with added noise\n",
    "#####################################################################\n",
    "\n",
    "resultr = evaluate_models(all_preds_r, targets_r)\n",
    "\n",
    "# View metrics\n",
    "print(f\"***************Individual Model Metrics********************************\")\n",
    "for m in resultr['individual_metrics']:\n",
    "    print(f\"Model {m['model_idx']+1} → R2: {m['r2']:.3f}, RMSE: {m['rmse']:.3f}, MAE: {m['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n",
    "\n",
    "print(f\"\\nBest Model : Model {resultr['best_model_index']+1}\")\n",
    "print(f\"***********************************************************************\")\n",
    "print(f\"Ensemble R2: {resultr['ensemble_metrics']['r2']:.3f}\")\n",
    "print(f\"Ensemble RMSE: {resultr['ensemble_metrics']['rmse']:.3f}\")\n",
    "print(f\"Ensemble MAE: {resultr['ensemble_metrics']['mae']:.3f}\")\n",
    "print(f\"***********************************************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7923b",
   "metadata": {},
   "source": [
    "### Table: Performance of different CAPRI-CT model variants\n",
    "\n",
    "| **Versions of CAPRI-CT model**                          | **MAE**   | **RMSE**   | **R²**   |\n",
    "|----------------------------------------------------------|-----------|------------|----------|\n",
    "| Image (*i*) + metadata (*v, t, a*)                       | 68.028    | 106.493    | 0.799    |\n",
    "| Image (*i*) + metadata (*v, t, a, noise*)                | 68.704    | 107.192    | 0.797    |\n",
    "| Image (*i*) + metadata (*v, a*)                          | 87.209    | 133.561    | 0.684    |\n",
    "| Image (*i*) + metadata (*t, a*)                          | 91.421    | 139.720    | 0.655    |\n",
    "| Image (*i*) only                                          | 172.722   | 235.216    | 0.021    |\n",
    "| Image (*i*) + metadata (*v, t*)                          | 164.256   | 237.137    | 0.005    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee6527",
   "metadata": {},
   "source": [
    "**Ablation Study and Causal Perturbation Analysis:**  \n",
    "The above table reports the results of causal structure perturbation through ablation studies, evaluating the impact of removing individual input variables on the model's performance. The full CAPRI-CT model achieves the best predictive accuracy (MAE: 68.03, RMSE: 106.49, R²: 0.799). Ablating current (*t*) and voltage (*v*) led to moderate degradation in performance, with R² decreasing to 0.684 and 0.655, respectively. This indicates their contribution to the prediction task but suggests limited causal influence relative to other variables.\n",
    "\n",
    "By contrast, removing the contrast agent (*a*) resulted in a substantial drop in performance (R²: 0.005), nearly equivalent to removing all three inputs (R²: 0.021). To further test model robustness, we introduced an additional noise variable to each input parameter; performance remained stable (MAE: 68.70, RMSE: 107.19, R²: 0.797). This highlights the contrast agent as a dominant causal factor for SNR in CT imaging. These results support the causal assumptions embedded in the model and demonstrate its sensitivity to disruptions in key parent nodes of the causal graph.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
